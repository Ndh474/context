{
  "metadata": {
    "codebase": "recognition-service",
    "category": "route",
    "generated_at": "2025-11-19T20:29:55.719713",
    "total_files": 6,
    "total_lines": 610,
    "total_bytes": 20298
  },
  "files": [
    {
      "path": "D:\\Work\\do_an_fall25\\fuacs\\recognition-service\\src\\recognition_service\\api\\v1\\__init__.py",
      "relative_path": "src/recognition_service/api/v1/__init__.py",
      "filename": "__init__.py",
      "size_bytes": 182,
      "lines": 10,
      "last_modified": "2025-10-28T12:06:40.942937",
      "encoding": "utf-8",
      "language": "python",
      "content": "\"\"\"\nAPI v1 routers\n\"\"\"\n\nfrom . import health\nfrom . import metrics\nfrom . import cameras\nfrom . import embeddings\n\n__all__ = [\"health\", \"metrics\", \"cameras\", \"embeddings\"]\n"
    },
    {
      "path": "D:\\Work\\do_an_fall25\\fuacs\\recognition-service\\src\\recognition_service\\api\\v1\\cameras.py",
      "relative_path": "src/recognition_service/api/v1/cameras.py",
      "filename": "cameras.py",
      "size_bytes": 4814,
      "lines": 133,
      "last_modified": "2025-10-28T11:39:41.837938",
      "encoding": "utf-8",
      "language": "python",
      "content": "\"\"\"\nCamera Endpoints\n\nGET /api/v1/cameras/test-connection - Test RTSP connection\nGET /api/v1/cameras/capture-frame - Capture preview frame\n\"\"\"\n\nfrom fastapi import APIRouter, Query, HTTPException, Depends\nfrom typing import Optional\nimport logging\n\nfrom recognition_service.models.responses import (\n    CameraTestResponse,\n    FrameCaptureResponse,\n    CameraTestData,\n    FrameCaptureData,\n)\nfrom recognition_service.services.camera_service import CameraService\nfrom recognition_service.services.rtsp_handler import RTSPConnectionError\nfrom recognition_service.core.security import verify_api_key\n\nlogger = logging.getLogger(__name__)\nrouter = APIRouter()\ncamera_service = CameraService()\n\n\n@router.get(\n    \"/cameras/test-connection\",\n    response_model=CameraTestResponse,\n    dependencies=[Depends(verify_api_key)],\n)\nasync def test_camera_connection(\n    rtspUrl: str = Query(..., description=\"RTSP stream URL\"),\n    timeout: int = Query(10, ge=1, le=30, description=\"Timeout (1-30s)\"),\n):\n    \"\"\"\n    Test RTSP camera connection.\n\n    Measures:\n    - Connection status\n    - Frame rate (FPS)\n    - Resolution\n    - Latency\n    - Stream stability\n\n    Requires X-API-Key header for authentication.\n    \"\"\"\n    logger.info(f\"Camera test requested for: {rtspUrl}\")\n\n    # Validate RTSP URL format\n    if not rtspUrl.startswith(\"rtsp://\"):\n        logger.warning(f\"Invalid RTSP URL format: {rtspUrl}\")\n        raise HTTPException(\n            status_code=400, detail=\"Invalid RTSP URL format. Must start with 'rtsp://'\"\n        )\n\n    try:\n        # Test connection\n        test_result = await camera_service.test_connection(rtspUrl, timeout)\n        data = CameraTestData(**test_result)\n\n        message = \"Camera connection successful\" if data.connected else \"Camera connection failed\"\n\n        logger.info(f\"Camera test completed - Connected: {data.connected}\")\n        return CameraTestResponse(status=200, message=message, data=data)\n\n    except Exception as e:\n        logger.error(f\"Unexpected error in camera test: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Camera test failed: {str(e)}\")\n\n\n@router.get(\n    \"/cameras/capture-frame\",\n    response_model=FrameCaptureResponse,\n    dependencies=[Depends(verify_api_key)],\n)\nasync def capture_camera_frame(\n    rtspUrl: Optional[str] = Query(None, description=\"RTSP stream URL\"),\n    cameraId: Optional[int] = Query(None, description=\"Camera ID\"),\n    format: str = Query(\"base64\", regex=\"^(base64|file)$\", description=\"Output format\"),\n    timeout: int = Query(10, ge=1, le=30, description=\"Timeout (1-30s)\"),\n):\n    \"\"\"\n    Capture single frame from camera.\n\n    Supports:\n    - base64 format (default) - returns image as base64 data URI\n    - file format - saves to temp/ directory and returns file path\n\n    Either rtspUrl or cameraId must be provided (but not both).\n    Requires X-API-Key header for authentication.\n    \"\"\"\n    logger.info(\n        f\"Frame capture requested - rtspUrl: {rtspUrl}, cameraId: {cameraId}, format: {format}\"\n    )\n\n    # Validate parameters\n    if not rtspUrl and not cameraId:\n        raise HTTPException(status_code=400, detail=\"Either rtspUrl or cameraId is required\")\n\n    if rtspUrl and cameraId:\n        raise HTTPException(status_code=400, detail=\"Provide either rtspUrl or cameraId, not both\")\n\n    # CameraId lookup not implemented yet (future enhancement)\n    if cameraId:\n        logger.warning(f\"CameraId lookup requested but not implemented: {cameraId}\")\n        raise HTTPException(\n            status_code=501, detail=\"CameraId lookup not implemented. Use rtspUrl parameter.\"\n        )\n\n    # Validate RTSP URL format\n    if rtspUrl and not rtspUrl.startswith(\"rtsp://\"):\n        logger.warning(f\"Invalid RTSP URL format: {rtspUrl}\")\n        raise HTTPException(\n            status_code=400, detail=\"Invalid RTSP URL format. Must start with 'rtsp://'\"\n        )\n\n    try:\n        # Capture frame\n        result = await camera_service.capture_frame(rtspUrl, format, cameraId, timeout)\n        data = FrameCaptureData(**result)\n\n        logger.info(\n            f\"Frame capture successful - Format: {format}, Resolution: {data.resolution.width}x{data.resolution.height}\"\n        )\n        return FrameCaptureResponse(status=200, message=\"Frame captured successfully\", data=data)\n\n    except RTSPConnectionError as e:\n        logger.error(f\"RTSP connection error during frame capture: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Frame capture failed: {str(e)}\")\n    except Exception as e:\n        logger.error(f\"Unexpected error during frame capture: {e}\")\n        raise HTTPException(status_code=500, detail=f\"Frame capture failed: {str(e)}\")\n"
    },
    {
      "path": "D:\\Work\\do_an_fall25\\fuacs\\recognition-service\\src\\recognition_service\\api\\v1\\embeddings.py",
      "relative_path": "src/recognition_service/api/v1/embeddings.py",
      "filename": "embeddings.py",
      "size_bytes": 5301,
      "lines": 165,
      "last_modified": "2025-11-03T15:22:44.552344",
      "encoding": "utf-8",
      "language": "python",
      "content": "\"\"\"\nEmbeddings Endpoints\n\nPOST /api/v1/embeddings/generate  - Generate face embedding from photo\n\"\"\"\n\nfrom fastapi import APIRouter, File, Form, UploadFile, Depends, HTTPException\nfrom pathlib import Path\nimport logging\nimport os\nimport time\nimport uuid\n\nfrom recognition_service.models.responses import (\n    EmbeddingGenerateResponse,\n    EmbeddingGenerateData,\n)\nfrom recognition_service.services.embedding_generator import (\n    EmbeddingGenerator,\n    FaceNotDetectedError,\n    LowQualityError,\n)\nfrom recognition_service.core.security import verify_api_key\nfrom recognition_service.core.config import get_settings\n\nlogger = logging.getLogger(__name__)\nrouter = APIRouter()\nsettings = get_settings()\n\n# Service instance will be created on first use to avoid model loading issues\nembedding_generator = None\n\n\ndef get_embedding_generator() -> EmbeddingGenerator:\n    \"\"\"Get or create embedding generator instance\"\"\"\n    global embedding_generator\n    if embedding_generator is None:\n        embedding_generator = EmbeddingGenerator()\n    return embedding_generator\n\n\n@router.post(\n    \"/embeddings/generate\",\n    response_model=EmbeddingGenerateResponse,\n    dependencies=[Depends(verify_api_key)],\n)\nasync def generate_embedding(\n    photo: UploadFile = File(..., description=\"Face photo file (JPG/PNG)\"),\n    submissionId: int = Form(..., description=\"Identity submission ID for logging\"),\n):\n    \"\"\"\n    Generate 512-dim face embedding from photo.\n\n    Process:\n    1. Upload photo via multipart/form-data\n    2. Extract embedding from photo\n    3. Calculate quality metrics\n    4. Validate quality >= threshold\n    5. Return 512-dim vector\n\n    Called by Java backend after identity submission approval.\n    \"\"\"\n    logger.info(f\"Embedding generation request: submissionId={submissionId}, photo={photo.filename}\")\n\n    # 1. Validate file exists\n    if not photo.filename:\n        return EmbeddingGenerateResponse(\n            status=400, message=\"Photo file is required\", code=\"PHOTO_FILE_REQUIRED\"\n        )\n\n    # 2. Validate format\n    allowed_formats = [\".jpg\", \".jpeg\", \".png\"]\n    ext = Path(photo.filename).suffix.lower()\n    if ext not in allowed_formats:\n        return EmbeddingGenerateResponse(\n            status=400,\n            message=f\"Invalid photo format. Allowed: {', '.join(allowed_formats)}\",\n            code=\"INVALID_FILE_FORMAT\",\n        )\n\n    # 3. Save to temp file\n    temp_filename = f\"generate_{uuid.uuid4()}{ext}\"\n    temp_path = os.path.join(settings.TEMP_DIR, temp_filename)\n\n    # Ensure temp directory exists\n    os.makedirs(settings.TEMP_DIR, exist_ok=True)\n\n    start_time = time.time()\n\n    try:\n        # 4. Write uploaded file to temp\n        with open(temp_path, \"wb\") as f:\n            content = await photo.read()\n            f.write(content)\n\n        logger.info(f\"Temp file saved: {temp_path}\")\n\n        # 5. Generate embedding from photo\n        result = await get_embedding_generator().generate_embedding_from_photo(\n            photo_path=temp_path,\n            submission_id=submissionId,\n        )\n\n        # 6. Add processing time\n        processing_time = time.time() - start_time\n        result[\"processingTime\"] = round(processing_time, 2)\n\n        # 7. Cleanup temp file\n        if os.path.exists(temp_path):\n            os.remove(temp_path)\n            logger.debug(f\"Temp file removed: {temp_path}\")\n\n        data = EmbeddingGenerateData(**result)\n\n        logger.info(\n            f\"Embedding generated: submissionId={submissionId}, \"\n            f\"quality={data.quality}, time={data.processingTime}s\"\n        )\n\n        return EmbeddingGenerateResponse(\n            status=200, message=\"Face embedding generated successfully\", data=data\n        )\n\n    except FileNotFoundError as e:\n        # Cleanup on error\n        if os.path.exists(temp_path):\n            os.remove(temp_path)\n\n        logger.error(f\"File not found: {e}\")\n        return EmbeddingGenerateResponse(\n            status=404, message=f\"Photo file not found: {str(e)}\", code=\"PHOTO_FILE_NOT_FOUND\"\n        )\n\n    except FaceNotDetectedError as e:\n        # Cleanup on error\n        if os.path.exists(temp_path):\n            os.remove(temp_path)\n\n        logger.warning(f\"Face not detected: {e}\")\n        return EmbeddingGenerateResponse(status=400, message=str(e), code=e.code)\n\n    except LowQualityError as e:\n        # Cleanup on error\n        if os.path.exists(temp_path):\n            os.remove(temp_path)\n\n        logger.warning(f\"Low quality: {e}\")\n        return EmbeddingGenerateResponse(\n            status=400,\n            message=f\"Face quality too low (score: {e.quality:.2f}). \"\n            \"Please take photo in better lighting and ensure face is clearly visible.\",\n            code=\"LOW_QUALITY_FACE\",\n        )\n\n    except Exception as e:\n        # Cleanup on error\n        if os.path.exists(temp_path):\n            os.remove(temp_path)\n\n        logger.error(f\"Embedding generation failed: {e}\", exc_info=True)\n        return EmbeddingGenerateResponse(\n            status=500,\n            message=f\"Failed to process photo: {str(e)}\",\n            code=\"IMAGE_PROCESSING_FAILED\",\n        )\n"
    },
    {
      "path": "D:\\Work\\do_an_fall25\\fuacs\\recognition-service\\src\\recognition_service\\api\\v1\\health.py",
      "relative_path": "src/recognition_service/api/v1/health.py",
      "filename": "health.py",
      "size_bytes": 2393,
      "lines": 79,
      "last_modified": "2025-11-06T18:21:52.073063",
      "encoding": "utf-8",
      "language": "python",
      "content": "\"\"\"\nHealth Check Endpoint\n\nGET /api/v1/health - Public endpoint for service health monitoring\n\nNo authentication required.\nSpring Boot PythonBackendClient.checkHealth() calls this endpoint.\n\"\"\"\n\nfrom fastapi import APIRouter\nfrom datetime import datetime\nimport logging\nimport pytz\n\nfrom recognition_service.models.responses import HealthResponse, HealthData\nfrom recognition_service.services.session_manager import session_manager\nfrom recognition_service.services.metrics_collector import metrics_collector\nfrom recognition_service.core.config import get_settings\n\nlogger = logging.getLogger(__name__)\nrouter = APIRouter()\n\n\n@router.get(\"/health\", response_model=HealthResponse)\nasync def health_check():\n    \"\"\"\n    Health check endpoint.\n\n    Returns service status, uptime, and active session count.\n    Used by load balancers and monitoring systems.\n\n    Returns:\n        HealthResponse: Service health information\n\n    Response Codes:\n        200: Service is healthy\n        503: Service is unhealthy (future implementation)\n\n    Example Response:\n        {\n          \"status\": 200,\n          \"message\": \"Service is healthy\",\n          \"data\": {\n            \"service\": \"FUACS Face Recognition Service\",\n            \"version\": \"1.0.0\",\n            \"status\": \"healthy\",\n            \"uptime\": 3600,\n            \"activeSessions\": 2,\n            \"timestamp\": \"2024-10-28T10:00:00Z\"\n          }\n        }\n    \"\"\"\n    settings = get_settings()\n\n    # Calculate uptime\n    vn_tz = pytz.timezone('Asia/Ho_Chi_Minh')\n    uptime = int((datetime.now(vn_tz) - metrics_collector.service_start_time).total_seconds())\n\n    # Determine health status\n    # For now, always return \"healthy\"\n    # Future: check if InsightFace model loaded, database connected, etc.\n    status = \"healthy\"\n\n    # Get active session count\n    active_sessions = session_manager.active_sessions_count\n\n    # Build response\n    data = HealthData(\n        service=settings.SERVICE_NAME,\n        version=settings.SERVICE_VERSION,\n        status=status,\n        uptime=uptime,\n        activeSessions=active_sessions,\n        timestamp=datetime.now(vn_tz).isoformat() + \"Z\",\n    )\n\n    logger.debug(f\"Health check: {status}, uptime={uptime}s, sessions={active_sessions}\")\n\n    return HealthResponse(status=200, message=\"Service is healthy\", data=data)\n"
    },
    {
      "path": "D:\\Work\\do_an_fall25\\fuacs\\recognition-service\\src\\recognition_service\\api\\v1\\metrics.py",
      "relative_path": "src/recognition_service/api/v1/metrics.py",
      "filename": "metrics.py",
      "size_bytes": 2568,
      "lines": 85,
      "last_modified": "2025-10-28T10:59:59.375524",
      "encoding": "utf-8",
      "language": "python",
      "content": "\"\"\"\nMetrics Endpoint\n\nGET /api/v1/metrics - Protected endpoint for performance metrics\n\nRequires X-API-Key header authentication.\nReturns aggregated statistics for monitoring and optimization.\n\"\"\"\n\nfrom fastapi import APIRouter, Depends\nimport logging\n\nfrom recognition_service.models.responses import MetricsResponse\nfrom recognition_service.services.metrics_collector import metrics_collector\nfrom recognition_service.core.security import verify_api_key\n\nlogger = logging.getLogger(__name__)\nrouter = APIRouter()\n\n\n@router.get(\"/metrics\", response_model=MetricsResponse, dependencies=[Depends(verify_api_key)])\nasync def get_metrics():\n    \"\"\"\n    Get performance metrics.\n\n    Returns comprehensive metrics including:\n    - Service uptime and session counts\n    - Face recognition statistics\n    - Embedding generation statistics\n    - Camera status\n\n    Security:\n        Requires X-API-Key header\n\n    Returns:\n        MetricsResponse: Performance metrics\n\n    Response Codes:\n        200: Metrics retrieved successfully\n        401: Invalid or missing API key\n\n    Example Response:\n        {\n          \"status\": 200,\n          \"message\": \"Metrics retrieved successfully\",\n          \"data\": {\n            \"service\": {\n              \"uptime\": 3600,\n              \"activeSessions\": 2,\n              \"totalSessionsToday\": 15\n            },\n            \"recognition\": {\n              \"totalRecognitions\": 450,\n              \"averageConfidence\": 0.91,\n              \"averageProcessingTime\": 0.12,\n              \"recognitionRate\": 0.87\n            },\n            \"embedding\": {\n              \"totalGenerated\": 25,\n              \"averageQuality\": 0.89,\n              \"averageProcessingTime\": 2.3,\n              \"successRate\": 0.96\n            },\n            \"cameras\": {\n              \"totalCameras\": 4,\n              \"activeCameras\": 4,\n              \"failedCameras\": 0,\n              \"averageFrameRate\": 24.5\n            },\n            \"timestamp\": \"2024-10-28T10:00:00Z\"\n          }\n        }\n    \"\"\"\n    logger.debug(\"Metrics endpoint called\")\n\n    # Get aggregated metrics from collector\n    metrics_data = metrics_collector.get_metrics_summary()\n\n    logger.info(\n        f\"Metrics: uptime={metrics_data['service']['uptime']}s, \"\n        f\"sessions={metrics_data['service']['activeSessions']}, \"\n        f\"recognitions={metrics_data['recognition']['totalRecognitions']}\"\n    )\n\n    return MetricsResponse(status=200, message=\"Metrics retrieved successfully\", data=metrics_data)\n"
    },
    {
      "path": "D:\\Work\\do_an_fall25\\fuacs\\recognition-service\\src\\recognition_service\\api\\v1\\recognition.py",
      "relative_path": "src/recognition_service/api/v1/recognition.py",
      "filename": "recognition.py",
      "size_bytes": 5040,
      "lines": 138,
      "last_modified": "2025-10-28T22:49:41.659695",
      "encoding": "utf-8",
      "language": "python",
      "content": "\"\"\"\nRecognition API Endpoints\nHandles face recognition session management\n\"\"\"\n\nfrom fastapi import APIRouter, Depends, HTTPException, status, Request\nfrom recognition_service.core.security import verify_api_key\nfrom recognition_service.models.recognition_requests import StartSessionRequest, StopSessionRequest\nfrom recognition_service.models.recognition_responses import RecognitionResponse, SessionDataDTO\nfrom recognition_service.services.recognition_service import recognition_service\nimport logging\nimport json\n\nlogger = logging.getLogger(__name__)\nrouter = APIRouter()\n\n\n@router.post(\"/process-session\", response_model=RecognitionResponse)\nasync def start_recognition_session(\n    raw_request: Request, request: StartSessionRequest, api_key: str = Depends(verify_api_key)\n):\n    \"\"\"\n    Start face recognition session for a slot\n\n    Process:\n    1. Validate request\n    2. Check no existing session\n    3. Test camera connections\n    4. Start background processing\n    5. Return session info\n    \"\"\"\n    try:\n        # Log incoming request details for debugging\n        logger.info(f\"=== INCOMING REQUEST DEBUG ===\")\n        logger.info(f\"Method: {raw_request.method}\")\n        logger.info(f\"URL: {raw_request.url}\")\n        logger.info(f\"Headers: {dict(raw_request.headers)}\")\n        logger.info(f\"Query params: {dict(raw_request.query_params)}\")\n        logger.info(f\"Path params: {dict(raw_request.path_params)}\")\n\n        # Log parsed request data\n        logger.info(f\"Parsed request - slotId: {request.slotId}, roomId: {request.roomId}, mode: {request.mode}\")\n        logger.info(f\"Students count: {len(request.students)}\")\n        logger.info(f\"Cameras count: {len(request.cameras)}\")\n        logger.info(\n            f\"Config: similarity={request.config.similarityThreshold}, interval={request.config.scanInterval}\"\n        )\n        logger.info(f\"Callback URL: {request.config.callbackUrl}\")\n        logger.info(f\"=== END REQUEST DEBUG ===\")\n\n        logger.info(f\"Starting recognition session for slot {request.slotId}\")\n\n        # Start session (async but returns immediately)\n        session_data = await recognition_service.start_session(request)\n\n        return RecognitionResponse(\n            status=200, message=\"Face recognition session started successfully\", data=session_data\n        )\n\n    except ValueError as e:\n        # Session already exists\n        logger.warning(f\"Session already exists for slot {request.slotId}\")\n        raise HTTPException(\n            status_code=409,\n            detail={\n                \"status\": 409,\n                \"message\": f\"Face recognition session already exists for slot {request.slotId}\",\n                \"code\": \"SESSION_ALREADY_EXISTS\",\n            },\n        )\n    except RuntimeError as e:\n        # All cameras failed\n        logger.error(f\"Failed to start session: {e}\")\n        raise HTTPException(\n            status_code=500,\n            detail={\n                \"status\": 500,\n                \"message\": \"Failed to connect to any camera. Cannot start session.\",\n                \"code\": \"ALL_CAMERAS_FAILED\",\n            },\n        )\n    except Exception as e:\n        logger.error(f\"Unexpected error starting session: {e}\")\n        raise HTTPException(\n            status_code=500,\n            detail={\n                \"status\": 500,\n                \"message\": f\"Internal server error: {str(e)}\",\n                \"code\": \"INTERNAL_ERROR\",\n            },\n        )\n\n\n@router.post(\"/stop-session\", response_model=RecognitionResponse)\nasync def stop_recognition_session(\n    request: StopSessionRequest, api_key: str = Depends(verify_api_key)\n):\n    \"\"\"\n    Stop face recognition session for a slot\n\n    Process:\n    1. Find active session\n    2. Stop all background tasks\n    3. Cleanup resources\n    4. Return statistics\n    \"\"\"\n    try:\n        logger.info(f\"Stopping recognition session for slot {request.slotId}\")\n\n        # Stop session and get statistics\n        session_data = await recognition_service.stop_session(request.slotId)\n\n        if session_data is None:\n            raise HTTPException(\n                status_code=404,\n                detail={\n                    \"status\": 404,\n                    \"message\": f\"No active face recognition session found for slot {request.slotId}\",\n                    \"code\": \"SESSION_NOT_FOUND\",\n                },\n            )\n\n        return RecognitionResponse(\n            status=200, message=\"Face recognition session stopped successfully\", data=session_data\n        )\n\n    except HTTPException:\n        raise  # Re-raise HTTP exceptions\n    except Exception as e:\n        logger.error(f\"Error stopping session: {e}\")\n        raise HTTPException(\n            status_code=500,\n            detail={\n                \"status\": 500,\n                \"message\": f\"Failed to stop session: {str(e)}\",\n                \"code\": \"STOP_SESSION_FAILED\",\n            },\n        )\n"
    }
  ]
}