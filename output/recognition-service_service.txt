{
  "metadata": {
    "codebase": "recognition-service",
    "category": "service",
    "generated_at": "2025-11-24T20:05:17.526862",
    "total_files": 15,
    "total_lines": 2275,
    "total_bytes": 78135
  },
  "files": [
    {
      "path": "D:\\Work\\do_an_fall25\\fuacs\\recognition-service\\src\\recognition_service\\services\\__init__.py",
      "relative_path": "src/recognition_service/services/__init__.py",
      "filename": "__init__.py",
      "size_bytes": 39,
      "lines": 1,
      "last_modified": "2025-10-28T10:59:59.203556",
      "encoding": "utf-8",
      "language": "python",
      "content": "\"\"\"Services package initialization\"\"\"\n"
    },
    {
      "path": "D:\\Work\\do_an_fall25\\fuacs\\recognition-service\\src\\recognition_service\\services\\callback_service.py",
      "relative_path": "src/recognition_service/services/callback_service.py",
      "filename": "callback_service.py",
      "size_bytes": 5447,
      "lines": 156,
      "last_modified": "2025-11-22T23:48:32.901989",
      "encoding": "utf-8",
      "language": "python",
      "content": "\"\"\"\nCallback Service\nSends recognition results back to Java backend\n\"\"\"\n\nimport aiohttp\nimport asyncio\nimport logging\nfrom typing import Dict, Any, List\nfrom datetime import datetime\nimport pytz\nfrom recognition_service.utils import get_utc_timestamp_for_java\n\nlogger = logging.getLogger(__name__)\n\nclass CallbackService:\n    \"\"\"\n    Handles callbacks to Java backend\n    \"\"\"\n\n    def __init__(self):\n        self.pending_callbacks: Dict[int, List[Dict]] = {}  # slot_id -> list of recognitions\n        self.batch_size = 100\n        self.retry_attempts = 3\n        self.retry_delay = 1  # seconds\n\n    async def send_recognition(\n        self,\n        callback_url: str,\n        slot_id: int,\n        recognition: Dict[str, Any],\n        mode: str = \"INITIAL\",\n        callback_type: str = \"REGULAR\"\n    ) -> bool:\n        \"\"\"\n        Send recognition result to Java backend\n\n        Args:\n            callback_url: Java backend endpoint\n            slot_id: Slot ID for this recognition\n            recognition: Recognition result data\n            mode: Scan mode (INITIAL or RESCAN)\n            callback_type: Callback routing type (\"REGULAR\" or \"EXAM\")\n\n        Returns:\n            True if successful, False otherwise\n        \"\"\"\n        # Add API key from settings\n        from recognition_service.core.config import get_settings\n        settings = get_settings()\n\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"X-API-Key\": settings.API_KEY\n        }\n\n        # Prepare request body matching Java backend contract\n        body = {\n            \"slotId\": slot_id,\n            \"mode\": mode,  # Include scan mode\n            \"callbackType\": callback_type,  # Include callback routing type\n            \"recognitions\": [recognition]\n        }\n\n        # Retry logic\n        for attempt in range(self.retry_attempts):\n            try:\n                async with aiohttp.ClientSession() as session:\n                    async with session.post(\n                        callback_url,\n                        json=body,\n                        headers=headers,\n                        timeout=aiohttp.ClientTimeout(total=30)\n                    ) as response:\n                        if response.status == 200:\n                            logger.debug(f\"CALLBACK_OK | slot={slot_id} student={recognition['studentUserId']}\")\n                            return True\n                        else:\n                            response_text = await response.text()\n                            logger.warning(\n                                f\"Callback failed with status {response.status}, \"\n                                f\"attempt {attempt + 1}/{self.retry_attempts}, \"\n                                f\"response: {response_text}\"\n                            )\n\n            except asyncio.TimeoutError:\n                logger.error(f\"Callback timeout, attempt {attempt + 1}/{self.retry_attempts}\")\n            except Exception as e:\n                logger.error(f\"Callback error: {e}, attempt {attempt + 1}/{self.retry_attempts}\")\n\n            # Exponential backoff\n            if attempt < self.retry_attempts - 1:\n                await asyncio.sleep(self.retry_delay * (2 ** attempt))\n\n        logger.error(f\"Failed to send callback after {self.retry_attempts} attempts\")\n        return False\n\n    async def batch_send_recognitions(\n        self,\n        callback_url: str,\n        recognitions: List[Dict[str, Any]]\n    ) -> bool:\n        \"\"\"\n        Send multiple recognitions in batch\n\n        Useful for sending accumulated results\n        \"\"\"\n        if not recognitions:\n            return True\n\n        from recognition_service.core.config import get_settings\n        settings = get_settings()\n\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"X-API-Key\": settings.API_KEY\n        }\n\n        # Split into batches if needed\n        for i in range(0, len(recognitions), self.batch_size):\n            batch = recognitions[i:i + self.batch_size]\n\n            body = {\n                \"recognitions\": batch,\n                \"timestamp\": get_utc_timestamp_for_java()\n            }\n\n            success = False\n            for attempt in range(self.retry_attempts):\n                try:\n                    async with aiohttp.ClientSession() as session:\n                        async with session.post(\n                            callback_url,\n                            json=body,\n                            headers=headers,\n                            timeout=aiohttp.ClientTimeout(total=30)\n                        ) as response:\n                            if response.status == 200:\n                                logger.info(f\"Batch callback successful: {len(batch)} recognitions\")\n                                success = True\n                                break\n\n                except Exception as e:\n                    logger.error(f\"Batch callback error: {e}\")\n\n                if attempt < self.retry_attempts - 1:\n                    await asyncio.sleep(self.retry_delay * (2 ** attempt))\n\n            if not success:\n                logger.error(f\"Failed to send batch of {len(batch)} recognitions\")\n                return False\n\n        return True\n\n# Singleton instance\ncallback_service = CallbackService()\n"
    },
    {
      "path": "D:\\Work\\do_an_fall25\\fuacs\\recognition-service\\src\\recognition_service\\services\\camera_service.py",
      "relative_path": "src/recognition_service/services/camera_service.py",
      "filename": "camera_service.py",
      "size_bytes": 4827,
      "lines": 143,
      "last_modified": "2025-11-22T23:33:03.097660",
      "encoding": "utf-8",
      "language": "python",
      "content": "\"\"\"\nCamera Service\n\nBusiness logic for camera testing and frame capture.\n\"\"\"\n\nimport logging\nimport base64\nimport time\nfrom datetime import datetime\nimport pytz\nfrom pathlib import Path\nimport cv2\n\nfrom recognition_service.services.rtsp_handler import RTSPHandler, RTSPConnectionError\n\nlogger = logging.getLogger(__name__)\n\n\nclass CameraService:\n    \"\"\"Camera testing and capture service.\"\"\"\n\n    def __init__(self, temp_dir: str = \"./temp\"):\n        self.temp_dir = Path(temp_dir)\n        self.temp_dir.mkdir(exist_ok=True)\n        logger.info(f\"Camera service initialized with temp directory: {self.temp_dir}\")\n\n    async def test_connection(self, rtsp_url: str, timeout: int = 10):\n        \"\"\"\n        Test RTSP camera connection.\n\n        Returns dict with:\n        - connected (bool)\n        - frameRate (float) if connected\n        - resolution (dict) if connected\n        - latency (int) if connected\n        - stability (str) if connected\n        - error (str) if failed\n        \"\"\"\n        logger.info(f\"Testing camera connection: {rtsp_url}\")\n\n        vn_tz = pytz.timezone('Asia/Ho_Chi_Minh')\n        result = {\n            \"rtspUrl\": rtsp_url,\n            \"connected\": False,\n            \"testedAt\": datetime.now(vn_tz).isoformat(),\n        }\n\n        try:\n            with RTSPHandler(rtsp_url, timeout) as handler:\n                result[\"connected\"] = True\n\n                # Measure FPS\n                frames, fps = handler.capture_frames(10)\n                result[\"frameRate\"] = round(fps, 1)\n\n                # Get resolution\n                width, height = handler.get_resolution()\n                result[\"resolution\"] = {\"width\": width, \"height\": height}\n\n                # Latency\n                result[\"latency\"] = handler.calculate_latency()\n\n                # Stability\n                result[\"stability\"] = self._check_stability(len(frames), 10)\n\n                logger.info(\n                    f\"Camera test successful - FPS: {fps:.1f}, Resolution: {width}x{height}\"\n                )\n\n        except RTSPConnectionError as e:\n            result[\"error\"] = str(e)\n            logger.warning(f\"Camera connection failed: {e}\")\n        except Exception as e:\n            result[\"error\"] = f\"Unexpected error: {str(e)}\"\n            logger.error(f\"Unexpected error during camera test: {e}\")\n\n        return result\n\n    async def capture_frame(\n        self, rtsp_url: str, format: str = \"base64\", camera_id: int = None, timeout: int = 10\n    ):\n        \"\"\"\n        Capture single frame from camera.\n\n        Args:\n            rtsp_url: RTSP stream URL\n            format: 'base64' or 'file'\n            camera_id: Optional camera ID\n            timeout: Connection timeout\n\n        Returns:\n            dict with captured frame data\n        \"\"\"\n        logger.info(f\"Capturing frame from camera: {rtsp_url}, format: {format}\")\n\n        vn_tz = pytz.timezone('Asia/Ho_Chi_Minh')\n        result = {\n            \"rtspUrl\": rtsp_url,\n            \"format\": format,\n            \"capturedAt\": datetime.now(vn_tz).isoformat(),\n        }\n\n        if camera_id:\n            result[\"cameraId\"] = camera_id\n\n        with RTSPHandler(rtsp_url, timeout) as handler:\n            frame = handler.capture_single_frame()\n\n            # Resolution\n            height, width = frame.shape[:2]\n            result[\"resolution\"] = {\"width\": width, \"height\": height}\n\n            # Encode based on format\n            if format == \"base64\":\n                # Encode to JPEG with maximum quality\n                _, buffer = cv2.imencode(\".jpg\", frame, [cv2.IMWRITE_JPEG_QUALITY, 100])\n                img_base64 = base64.b64encode(buffer).decode(\"utf-8\")\n                result[\"image\"] = f\"data:image/jpeg;base64,{img_base64}\"\n                logger.debug(f\"Frame encoded to base64, size: {len(img_base64)} chars\")\n\n            elif format == \"file\":\n                filename = f\"camera_preview_{int(time.time())}.jpg\"\n                filepath = self.temp_dir / filename\n                cv2.imwrite(str(filepath), frame)\n                result[\"filePath\"] = str(filepath)\n                result[\"fileUrl\"] = f\"/api/v1/files/{filename}\"\n                logger.info(f\"Frame saved to file: {filepath}\")\n\n        return result\n\n    def _check_stability(self, captured: int, expected: int) -> str:\n        \"\"\"Check stream stability based on frame drops.\"\"\"\n        if expected == 0:\n            return \"unknown\"\n\n        drop_rate = 1.0 - (captured / expected)\n        stability = \"stable\" if drop_rate < 0.1 else \"unstable\"\n\n        logger.debug(\n            f\"Stream stability: {stability} (captured {captured}/{expected}, drop rate: {drop_rate:.1%})\"\n        )\n        return stability\n"
    },
    {
      "path": "D:\\Work\\do_an_fall25\\fuacs\\recognition-service\\src\\recognition_service\\services\\embedding_generator.py",
      "relative_path": "src/recognition_service/services/embedding_generator.py",
      "filename": "embedding_generator.py",
      "size_bytes": 8594,
      "lines": 251,
      "last_modified": "2025-11-03T15:21:00.183640",
      "encoding": "utf-8",
      "language": "python",
      "content": "\"\"\"\nEmbedding Generator Service - Generate embeddings with validation\n\"\"\"\n\nimport cv2\nimport numpy as np\nimport logging\nfrom typing import Dict, List, Tuple\nfrom recognition_service.services.face_encoder import FaceEncoder\nfrom recognition_service.services.quality_analyzer import QualityAnalyzer\nfrom recognition_service.services.file_handler import FileHandler\nfrom recognition_service.utils.similarity import cosine_similarity\nfrom recognition_service.core.config import get_settings\n\nlogger = logging.getLogger(__name__)\n\n\n# Custom Exceptions\nclass FaceNotDetectedError(Exception):\n    \"\"\"Raised when no face is detected in photo or video\"\"\"\n\n    def __init__(self, message: str, code: str):\n        super().__init__(message)\n        self.code = code\n\n\nclass FaceMismatchError(Exception):\n    \"\"\"Raised when photo and video faces don't match\"\"\"\n\n    pass\n\n\nclass LowQualityError(Exception):\n    \"\"\"Raised when embedding quality is too low\"\"\"\n\n    def __init__(self, quality: float):\n        super().__init__(f\"Quality too low: {quality}\")\n        self.quality = quality\n\n\nclass EmbeddingGenerator:\n    \"\"\"\n    Generate face embeddings from photo + video.\n\n    Validates:\n    - Face exists in both photo and video\n    - Photo-video similarity >= 0.90\n    - Embedding quality >= 0.50\n    \"\"\"\n\n    def __init__(self):\n        self.face_encoder = FaceEncoder()\n        self.quality_analyzer = QualityAnalyzer()\n        self.file_handler = FileHandler()\n        self.settings = get_settings()\n\n    async def generate_embedding(\n        self, photo_path: str, video_path: str, submission_id: int\n    ) -> Dict:\n        \"\"\"\n        Generate 512-dim embedding from photo + video.\n\n        Process:\n        1. Load photo → extract embedding\n        2. Load video → extract embeddings from frames\n        3. Validate photo-video similarity (>= 0.90)\n        4. Select best quality embedding from video\n        5. Return embedding + metadata\n\n        Args:\n            photo_path: Path to static face photo\n            video_path: Path to face video\n            submission_id: Identity submission ID (for logging)\n\n        Returns:\n            dict: {\n                'submissionId': int,\n                'embeddingVector': [512 floats],\n                'quality': float,\n                'faceDetected': bool,\n                'totalFrames': int,\n                'framesWithFace': int\n            }\n\n        Raises:\n            FaceNotDetectedError: No face in photo/video\n            FaceMismatchError: Photo and video faces don't match\n            LowQualityError: Embedding quality too low\n        \"\"\"\n        logger.info(f\"Generating embedding for submissionId={submission_id}\")\n\n        # Step 1: Load photo and extract embedding\n        photo_frame = await self.file_handler.load_image(photo_path)\n        photo_embedding = self.face_encoder.extract_embedding(photo_frame)\n\n        if photo_embedding is None:\n            raise FaceNotDetectedError(\n                \"No face detected in the provided static photo.\", \"NO_FACE_IN_PHOTO\"\n            )\n\n        logger.info(\"Photo embedding extracted successfully\")\n\n        # Step 2: Extract embeddings from video\n        video_embeddings, total_frames = await self._extract_video_embeddings(video_path)\n\n        if len(video_embeddings) == 0:\n            raise FaceNotDetectedError(\n                \"No face detected in the provided video.\", \"NO_FACE_IN_VIDEO\"\n            )\n\n        logger.info(f\"Video embeddings extracted: {len(video_embeddings)}/{total_frames} frames\")\n\n        # Step 3: Validate photo-video similarity\n        avg_video_embedding = np.mean([e[\"embedding\"] for e in video_embeddings], axis=0)\n        similarity = cosine_similarity(photo_embedding, avg_video_embedding)\n\n        logger.info(f\"Photo-video similarity: {similarity:.4f}\")\n\n        if similarity < self.settings.EMBEDDING_VALIDATION_THRESHOLD:\n            raise FaceMismatchError(\n                f\"Similarity {similarity:.2f} < threshold \"\n                f\"{self.settings.EMBEDDING_VALIDATION_THRESHOLD}\"\n            )\n\n        # Step 4: Select best quality embedding\n        best = max(video_embeddings, key=lambda x: x[\"quality\"])\n\n        logger.info(f\"Best embedding quality: {best['quality']:.2f}\")\n\n        if best[\"quality\"] < self.settings.EMBEDDING_QUALITY_THRESHOLD:\n            raise LowQualityError(best[\"quality\"])\n\n        # Step 5: Return result\n        return {\n            \"submissionId\": submission_id,\n            \"embeddingVector\": best[\"embedding\"].tolist(),  # Convert numpy to list\n            \"quality\": round(best[\"quality\"], 2),\n            \"faceDetected\": True,\n            \"totalFrames\": total_frames,\n            \"framesWithFace\": len(video_embeddings),\n        }\n\n    async def _extract_video_embeddings(self, video_path: str) -> Tuple[List[Dict], int]:\n        \"\"\"\n        Extract embeddings from video frames.\n\n        Returns:\n            (embeddings_list, total_frames)\n            embeddings_list: [{'embedding': ndarray, 'quality': float}, ...]\n        \"\"\"\n        cap = await self.file_handler.load_video(video_path)\n\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        interval = self.settings.VIDEO_SAMPLE_INTERVAL\n        frame_interval = int(fps * interval)  # Sample every 0.5 seconds\n\n        embeddings = []\n        frame_count = 0\n        total_frames = 0\n\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            total_frames += 1\n\n            # Sample at interval\n            if frame_count % frame_interval == 0:\n                # Detect face\n                faces = self.face_encoder.detect_faces(frame)\n\n                if len(faces) > 0:\n                    face = faces[0]\n                    embedding = face.embedding  # 512-dim numpy array\n\n                    # Calculate quality\n                    metrics = self.quality_analyzer.calculate_metrics(frame, face)\n                    quality = self.quality_analyzer.calculate_overall_quality(metrics)\n\n                    embeddings.append({\"embedding\": embedding, \"quality\": quality})\n\n            frame_count += 1\n\n        cap.release()\n\n        logger.debug(f\"Extracted {len(embeddings)} embeddings from {total_frames} frames\")\n\n        return embeddings, total_frames\n\n    async def generate_embedding_from_photo(self, photo_path: str, submission_id: int) -> Dict:\n        \"\"\"\n        Generate 512-dim embedding from photo only.\n\n        Process:\n        1. Load photo → extract embedding\n        2. Calculate quality metrics\n        3. Validate quality >= threshold\n        4. Return embedding + metadata\n\n        Args:\n            photo_path: Path to static face photo\n            submission_id: Identity submission ID (for logging)\n\n        Returns:\n            dict: {\n                'submissionId': int,\n                'embeddingVector': [512 floats],\n                'quality': float,\n                'faceDetected': bool\n            }\n\n        Raises:\n            FaceNotDetectedError: No face in photo\n            LowQualityError: Embedding quality too low\n        \"\"\"\n        logger.info(f\"Generating embedding from photo for submissionId={submission_id}\")\n\n        # Step 1: Load photo and extract embedding\n        photo_frame = await self.file_handler.load_image(photo_path)\n\n        # Step 2: Detect faces\n        faces = self.face_encoder.detect_faces(photo_frame)\n\n        if len(faces) == 0:\n            raise FaceNotDetectedError(\n                \"No face detected in the provided photo.\", \"NO_FACE_IN_PHOTO\"\n            )\n\n        face = faces[0]\n        embedding = face.embedding  # 512-dim numpy array\n\n        logger.info(\"Photo embedding extracted successfully\")\n\n        # Step 3: Calculate quality metrics\n        metrics = self.quality_analyzer.calculate_metrics(photo_frame, face)\n        quality = self.quality_analyzer.calculate_overall_quality(metrics)\n\n        logger.info(f\"Embedding quality: {quality:.2f}\")\n\n        # Step 4: Validate quality\n        if quality < self.settings.EMBEDDING_QUALITY_THRESHOLD:\n            raise LowQualityError(quality)\n\n        # Step 5: Return result\n        return {\n            \"submissionId\": submission_id,\n            \"embeddingVector\": embedding.tolist(),  # Convert numpy to list\n            \"quality\": round(quality, 2),\n            \"faceDetected\": True,\n        }\n"
    },
    {
      "path": "D:\\Work\\do_an_fall25\\fuacs\\recognition-service\\src\\recognition_service\\services\\face_encoder.py",
      "relative_path": "src/recognition_service/services/face_encoder.py",
      "filename": "face_encoder.py",
      "size_bytes": 2911,
      "lines": 89,
      "last_modified": "2025-10-28T12:14:08.704001",
      "encoding": "utf-8",
      "language": "python",
      "content": "\"\"\"\nFace Encoder Service - Wrapper for InsightFace operations\n\"\"\"\n\nfrom insightface.app import FaceAnalysis\nfrom recognition_service.services.model_loader import get_face_app\nimport numpy as np\nimport logging\nfrom typing import Optional\n\nlogger = logging.getLogger(__name__)\n\n\nclass FaceEncoder:\n    \"\"\"\n    Facade for InsightFace face detection and encoding.\n\n    Provides simplified interface for:\n    - Detecting faces in frames\n    - Extracting 512-dim embeddings\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"Initialize with lazy loading\"\"\"\n        self.app: Optional[FaceAnalysis] = None\n        logger.info(\"FaceEncoder initialized (model will be loaded on first use)\")\n\n    def _ensure_model_loaded(self):\n        \"\"\"Ensure the InsightFace model is loaded\"\"\"\n        if self.app is None:\n            self.app = get_face_app()\n\n    def detect_faces(self, frame: np.ndarray) -> list:\n        \"\"\"\n        Detect faces in a frame.\n\n        Args:\n            frame: BGR image as numpy array (from cv2.imread or cv2.VideoCapture)\n\n        Returns:\n            List of Face objects with attributes:\n            - bbox: [x1, y1, x2, y2]\n            - kps: 5 facial keypoints\n            - embedding: 512-dim numpy array\n            - det_score: detection confidence\n            - age, gender, etc.\n\n        Example:\n            >>> frame = cv2.imread(\"photo.jpg\")\n            >>> faces = encoder.detect_faces(frame)\n            >>> if len(faces) > 0:\n            ...     print(f\"Found {len(faces)} face(s)\")\n            ...     print(f\"Embedding shape: {faces[0].embedding.shape}\")  # (512,)\n        \"\"\"\n        try:\n            self._ensure_model_loaded()\n            faces = self.app.get(frame)\n            logger.debug(f\"Detected {len(faces)} face(s) in frame\")\n            return faces\n        except Exception as e:\n            logger.error(f\"Face detection failed: {e}\")\n            return []\n\n    def extract_embedding(self, frame: np.ndarray) -> Optional[np.ndarray]:\n        \"\"\"\n        Extract face embedding from frame (first face only).\n\n        Args:\n            frame: BGR image as numpy array\n\n        Returns:\n            512-dim embedding vector (numpy array), or None if no face detected\n\n        Example:\n            >>> frame = cv2.imread(\"photo.jpg\")\n            >>> embedding = encoder.extract_embedding(frame)\n            >>> if embedding is not None:\n            ...     print(f\"Embedding shape: {embedding.shape}\")  # (512,)\n        \"\"\"\n        faces = self.detect_faces(frame)\n\n        if len(faces) == 0:\n            logger.warning(\"No face detected in frame\")\n            return None\n\n        # Return first face's embedding (already 512-dim with buffalo_l)\n        embedding = faces[0].embedding\n        logger.debug(f\"Extracted embedding with shape {embedding.shape}\")\n        return embedding\n"
    },
    {
      "path": "D:\\Work\\do_an_fall25\\fuacs\\recognition-service\\src\\recognition_service\\services\\face_recognizer.py",
      "relative_path": "src/recognition_service/services/face_recognizer.py",
      "filename": "face_recognizer.py",
      "size_bytes": 10157,
      "lines": 259,
      "last_modified": "2025-11-23T00:22:39.754922",
      "encoding": "utf-8",
      "language": "python",
      "content": "\"\"\"\nFace Recognizer Service\nDetects faces and matches with student embeddings\n\"\"\"\n\nimport cv2\nimport numpy as np\nimport logging\nfrom typing import List, Dict, Any, Optional\nfrom datetime import datetime\nimport pytz\nfrom insightface.app import FaceAnalysis\nimport os\nfrom recognition_service.core.hardware import (\n    get_onnx_providers,\n    get_insightface_ctx_id,\n    get_device_info,\n)\nfrom recognition_service.utils import get_utc_timestamp_for_java\n\nlogger = logging.getLogger(__name__)\n\n\nclass FaceRecognizer:\n    \"\"\"\n    Face detection and recognition using InsightFace\n    \"\"\"\n\n    def __init__(self):\n        self.face_app = None\n        self._init_model()\n\n    def _init_model(self):\n        \"\"\"Initialize InsightFace model with auto-detected hardware\"\"\"\n        try:\n            providers = get_onnx_providers()\n            ctx_id = get_insightface_ctx_id()\n\n            logger.info(f\"FaceRecognizer - Hardware: {get_device_info()}\")\n            logger.info(f\"FaceRecognizer - ONNX Providers: {providers}\")\n\n            self.face_app = FaceAnalysis(name=\"buffalo_l\", providers=providers)\n            self.face_app.prepare(ctx_id=ctx_id, det_size=(640, 640))\n            logger.info(f\"FaceRecognizer initialized with buffalo_l model on {get_device_info()}\")\n        except Exception as e:\n            logger.error(f\"Failed to initialize FaceRecognizer: {e}\")\n\n    async def process_frame(\n        self, frame: np.ndarray, students: List[Any], similarity_threshold: float, slot_id: int, camera_id: int,\n        recognized_students: set,\n        callback_type: str = \"REGULAR\"\n    ) -> List[Dict]:\n        \"\"\"\n        Process single frame for face recognition\n\n        Args:\n            recognized_students: Set of student_user_ids already recognized in this session (for deduplication)\n            callback_type: Type of callback routing (\"REGULAR\" or \"EXAM\")\n\n        Returns list of recognition results\n        \"\"\"\n        if self.face_app is None:\n            logger.error(\"Face model not initialized\")\n            return []\n\n        recognitions = []\n\n        try:\n            # Validate frame\n            if frame is None or frame.size == 0:\n                logger.warning(\"Invalid frame received (None or empty)\")\n                return []\n\n            frame_shape = frame.shape\n            logger.debug(f\"Processing frame: shape={frame_shape}, dtype={frame.dtype}\")\n\n            # Detect faces in frame\n            faces = self.face_app.get(frame)\n\n            if not faces:\n                logger.debug(f\"No faces detected in frame for slot {slot_id}, camera {camera_id}\")\n                return []\n\n            logger.debug(f\"Detected {len(faces)} face(s) in frame for slot {slot_id}, camera {camera_id}\")\n\n            # Process each detected face\n            for face in faces:\n                # Get face embedding (512-dim)\n                face_embedding = face.normed_embedding\n\n                # Find best match among students\n                best_match = self._find_best_match(face_embedding, students, similarity_threshold)\n\n                if best_match:\n                    student_id = best_match['userId']\n\n                    # Deduplication check: Skip if already recognized in this session\n                    if student_id in recognized_students:\n                        logger.debug(f\"Student {student_id} already recognized, skipping\")\n                        continue\n\n                    # Crop face from frame\n                    bbox = face.bbox.astype(int)\n                    x1, y1, x2, y2 = bbox\n\n                    # Add padding (increased for better context)\n                    padding = 50\n                    h, w = frame.shape[:2]\n                    x1 = max(0, x1 - padding)\n                    y1 = max(0, y1 - padding)\n                    x2 = min(w, x2 + padding)\n                    y2 = min(h, y2 + padding)\n\n                    face_crop = frame[y1:y2, x1:x2]\n\n                    # Ensure minimum size 300x300 for better quality\n                    min_size = 300\n                    crop_h, crop_w = face_crop.shape[:2]\n                    if crop_h < min_size or crop_w < min_size:\n                        # Calculate scale to reach minimum size\n                        scale = max(min_size / crop_h, min_size / crop_w)\n                        new_w = int(crop_w * scale)\n                        new_h = int(crop_h * scale)\n                        face_crop = cv2.resize(face_crop, (new_w, new_h), interpolation=cv2.INTER_LANCZOS4)\n\n                    # Save evidence image with user_id and roll_number\n                    evidence_path = self._save_evidence(\n                        face_crop, slot_id, student_id, best_match[\"rollNumber\"], callback_type\n                    )\n\n                    # Create recognition result matching Java backend contract\n                    # Format timestamp in UTC with Z suffix for Java Instant parsing\n                    timestamp = get_utc_timestamp_for_java()\n\n                    # Conditional evidence URL based on callback type\n                    recognition = {\n                        \"studentUserId\": student_id,\n                        \"confidence\": best_match[\"similarity\"],\n                        \"timestamp\": timestamp,\n                        \"cameraId\": camera_id,\n                        \"evidence\": {\n                            \"regularImageUrl\": evidence_path if callback_type == \"REGULAR\" else None,\n                            \"examImageUrl\": evidence_path if callback_type == \"EXAM\" else None\n                        }\n                    }\n\n                    recognitions.append(recognition)\n\n                else:\n                    logger.debug(\n                        f\"Face detected but no match found (threshold: {similarity_threshold})\"\n                    )\n\n        except Exception as e:\n            logger.error(f\"Error processing frame: {e}\", exc_info=True)\n\n        return recognitions\n\n    def _find_best_match(\n        self, face_embedding: np.ndarray, students: List[Any], threshold: float\n    ) -> Optional[Dict]:\n        \"\"\"\n        Find best matching student using cosine similarity\n        \"\"\"\n        logger.debug(f\"Matching: emb_dim={face_embedding.shape[0]}, students={len(students)}, threshold={threshold}\")\n\n        best_match = None\n        best_similarity = 0\n        similarity_scores = []\n\n        for student in students:\n            # Convert student embedding to numpy array\n            student_embedding = np.array(student.embeddingVector)\n\n            # Check dimension compatibility\n            if face_embedding.shape[0] != student_embedding.shape[0]:\n                logger.warning(\n                    f\"Dimension mismatch! Face: {face_embedding.shape[0]}, \"\n                    f\"Student {student.userId}: {student_embedding.shape[0]}\"\n                )\n                continue\n\n            # Calculate cosine similarity\n            similarity = self._cosine_similarity(face_embedding, student_embedding)\n            similarity_scores.append({\n                \"userId\": student.userId,\n                \"fullName\": student.fullName,\n                \"similarity\": float(similarity)\n            })\n\n            logger.debug(\n                f\"Student {student.userId} ({student.fullName}): similarity={similarity:.4f}\"\n            )\n\n            # Track best similarity regardless of threshold\n            if similarity > best_similarity:\n                best_similarity = similarity\n\n                # Only set as match if exceeds threshold\n                if similarity >= threshold:\n                    best_match = {\n                        \"userId\": student.userId,\n                        \"fullName\": student.fullName,\n                        \"rollNumber\": student.rollNumber,\n                        \"similarity\": float(similarity),\n                    }\n\n        # Log summary (DEBUG level to reduce noise)\n        if similarity_scores:\n            sorted_scores = sorted(similarity_scores, key=lambda x: x['similarity'], reverse=True)\n            top_3 = sorted_scores[:3]\n            top_3_str = \", \".join([f\"{s['fullName']}={s['similarity']:.4f}\" for s in top_3])\n            logger.debug(f\"Top 3: [{top_3_str}], threshold={threshold}\")\n\n        if not best_match:\n            logger.debug(f\"No match: best={best_similarity:.4f}, threshold={threshold}\")\n\n        return best_match\n\n    def _cosine_similarity(self, vec1: np.ndarray, vec2: np.ndarray) -> float:\n        \"\"\"Calculate cosine similarity between two vectors\"\"\"\n        dot_product = np.dot(vec1, vec2)\n        norm1 = np.linalg.norm(vec1)\n        norm2 = np.linalg.norm(vec2)\n\n        if norm1 == 0 or norm2 == 0:\n            return 0.0\n\n        return dot_product / (norm1 * norm2)\n\n    def _save_evidence(self, face_crop: np.ndarray, slot_id: int, user_id: int, roll_number: str, callback_type: str = \"REGULAR\") -> str:\n        \"\"\"Save cropped face as evidence and return full URL\"\"\"\n        from recognition_service.core.config import get_settings\n\n        # Add _exam postfix for exam attendance evidence\n        if callback_type == \"EXAM\":\n            filename = f\"{user_id}_{roll_number}_exam.jpg\"\n        else:\n            filename = f\"{user_id}_{roll_number}.jpg\"\n\n        # Create directory if not exists\n        evidence_dir = f\"./uploads/evidence/{slot_id}\"\n        os.makedirs(evidence_dir, exist_ok=True)\n\n        # Full path\n        filepath = os.path.join(evidence_dir, filename)\n\n        # Save image with maximum quality\n        cv2.imwrite(filepath, face_crop, [cv2.IMWRITE_JPEG_QUALITY, 100])\n\n        # Return full URL for backend to download\n        # Use PUBLIC_HOST instead of HOST (0.0.0.0 is not routable)\n        settings = get_settings()\n        base_url = f\"http://{settings.PUBLIC_HOST}:{settings.PORT}\"\n        return f\"{base_url}/uploads/evidence/{slot_id}/{filename}\"\n\n\n# Singleton instance\nface_recognizer = FaceRecognizer()\n"
    },
    {
      "path": "D:\\Work\\do_an_fall25\\fuacs\\recognition-service\\src\\recognition_service\\services\\file_handler.py",
      "relative_path": "src/recognition_service/services/file_handler.py",
      "filename": "file_handler.py",
      "size_bytes": 2516,
      "lines": 87,
      "last_modified": "2025-10-28T12:06:40.388816",
      "encoding": "utf-8",
      "language": "python",
      "content": "\"\"\"\nFile Handler Service - Load files from local paths\n\"\"\"\n\nimport cv2\nimport numpy as np\nfrom pathlib import Path\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass FileHandler:\n    \"\"\"\n    Load images and videos from local file paths.\n\n    Supports:\n    - Absolute paths: /uploads/identity/201/photo.jpg\n    - Relative paths: uploads/identity/201/photo.jpg\n\n    Future: HTTP/HTTPS URLs\n    \"\"\"\n\n    async def load_image(self, path: str) -> np.ndarray:\n        \"\"\"\n        Load image from local file path.\n\n        Args:\n            path: File path (absolute or relative)\n\n        Returns:\n            BGR image as numpy array\n\n        Raises:\n            FileNotFoundError: If file doesn't exist\n            ValueError: If file cannot be read as image\n\n        Example:\n            >>> handler = FileHandler()\n            >>> frame = await handler.load_image(\"/uploads/identity/1/photo.jpg\")\n            >>> print(frame.shape)  # (height, width, 3)\n        \"\"\"\n        logger.info(f\"Loading image from: {path}\")\n\n        # Check file exists\n        if not Path(path).exists():\n            logger.error(f\"File not found: {path}\")\n            raise FileNotFoundError(f\"Photo file not found: {path}\")\n\n        # Read image\n        frame = cv2.imread(path)\n        if frame is None:\n            logger.error(f\"Failed to read image: {path}\")\n            raise ValueError(f\"Failed to read image file: {path}\")\n\n        logger.info(f\"Image loaded successfully: {frame.shape}\")\n        return frame\n\n    async def load_video(self, path: str) -> cv2.VideoCapture:\n        \"\"\"\n        Open video file for reading.\n\n        Args:\n            path: Video file path\n\n        Returns:\n            cv2.VideoCapture object\n\n        Raises:\n            FileNotFoundError: If file doesn't exist or cannot be opened\n        \"\"\"\n        logger.info(f\"Opening video from: {path}\")\n\n        if not Path(path).exists():\n            logger.error(f\"Video file not found: {path}\")\n            raise FileNotFoundError(f\"Video file not found: {path}\")\n\n        cap = cv2.VideoCapture(path)\n        if not cap.isOpened():\n            logger.error(f\"Failed to open video: {path}\")\n            raise FileNotFoundError(f\"Failed to open video file: {path}\")\n\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n        logger.info(f\"Video opened: {fps} FPS, {frame_count} frames\")\n\n        return cap\n"
    },
    {
      "path": "D:\\Work\\do_an_fall25\\fuacs\\recognition-service\\src\\recognition_service\\services\\metrics_collector.py",
      "relative_path": "src/recognition_service/services/metrics_collector.py",
      "filename": "metrics_collector.py",
      "size_bytes": 7297,
      "lines": 219,
      "last_modified": "2025-11-06T18:22:27.363279",
      "encoding": "utf-8",
      "language": "python",
      "content": "\"\"\"\nMetrics Collector\n\nCollects performance metrics for recognition, embeddings, and cameras.\nProvides aggregated statistics for the /metrics endpoint.\n\nNote: Metrics are stored in-memory. For production, consider:\n- Time-series database (InfluxDB, TimescaleDB)\n- Metrics retention policy\n- Aggregation windows (hourly, daily)\n\"\"\"\n\nfrom dataclasses import dataclass\nfrom datetime import datetime, timedelta\nfrom typing import List, Dict\nimport asyncio\nfrom statistics import mean\nimport pytz\n\n\n@dataclass\nclass RecognitionMetric:\n    \"\"\"Single recognition event metric\"\"\"\n\n    timestamp: datetime\n    confidence: float\n    processing_time: float\n\n\n@dataclass\nclass EmbeddingMetric:\n    \"\"\"Single embedding generation metric\"\"\"\n\n    timestamp: datetime\n    quality: float\n    processing_time: float\n    success: bool\n\n\n@dataclass\nclass CameraMetric:\n    \"\"\"Camera status metric\"\"\"\n\n    camera_id: int\n    frame_rate: float\n    is_active: bool\n\n\nclass MetricsCollector:\n    \"\"\"\n    Collects and aggregates performance metrics.\n\n    Singleton instance for collecting metrics across the application.\n    \"\"\"\n\n    def __init__(self):\n        vn_tz = pytz.timezone('Asia/Ho_Chi_Minh')\n        self.service_start_time = datetime.now(vn_tz)\n        self._recognition_metrics: List[RecognitionMetric] = []\n        self._embedding_metrics: List[EmbeddingMetric] = []\n        self._camera_metrics: Dict[int, CameraMetric] = {}\n        self._total_sessions_today = 0\n        self._lock = asyncio.Lock()\n\n    async def record_recognition(self, confidence: float, processing_time: float) -> None:\n        \"\"\"\n        Record a face recognition event.\n\n        Args:\n            confidence: Similarity score (0.0-1.0)\n            processing_time: Processing time in seconds\n        \"\"\"\n        async with self._lock:\n            vn_tz = pytz.timezone('Asia/Ho_Chi_Minh')\n            self._recognition_metrics.append(\n                RecognitionMetric(\n                    timestamp=datetime.now(vn_tz),\n                    confidence=confidence,\n                    processing_time=processing_time,\n                )\n            )\n\n    async def record_embedding(self, quality: float, processing_time: float, success: bool) -> None:\n        \"\"\"\n        Record an embedding generation event.\n\n        Args:\n            quality: Quality score (0.0-1.0)\n            processing_time: Processing time in seconds\n            success: Whether generation succeeded\n        \"\"\"\n        async with self._lock:\n            vn_tz = pytz.timezone('Asia/Ho_Chi_Minh')\n            self._embedding_metrics.append(\n                EmbeddingMetric(\n                    timestamp=datetime.now(vn_tz),\n                    quality=quality,\n                    processing_time=processing_time,\n                    success=success,\n                )\n            )\n\n    async def update_camera(self, camera_id: int, frame_rate: float, is_active: bool) -> None:\n        \"\"\"\n        Update camera status.\n\n        Args:\n            camera_id: Camera ID\n            frame_rate: Current FPS\n            is_active: Whether camera is active\n        \"\"\"\n        async with self._lock:\n            self._camera_metrics[camera_id] = CameraMetric(\n                camera_id=camera_id, frame_rate=frame_rate, is_active=is_active\n            )\n\n    async def increment_sessions_today(self) -> None:\n        \"\"\"Increment total sessions counter\"\"\"\n        async with self._lock:\n            self._total_sessions_today += 1\n\n    def get_metrics_summary(self) -> dict:\n        \"\"\"\n        Get aggregated metrics summary.\n\n        Returns:\n            dict: Metrics data for /metrics endpoint\n        \"\"\"\n        from recognition_service.services.session_manager import session_manager\n\n        vn_tz = pytz.timezone('Asia/Ho_Chi_Minh')\n        return {\n            \"service\": {\n                \"uptime\": int((datetime.now(vn_tz) - self.service_start_time).total_seconds()),\n                \"activeSessions\": session_manager.active_sessions_count,\n                \"totalSessionsToday\": self._total_sessions_today,\n            },\n            \"recognition\": self._get_recognition_stats(),\n            \"embedding\": self._get_embedding_stats(),\n            \"cameras\": self._get_camera_stats(),\n            \"timestamp\": datetime.now(vn_tz).isoformat() + \"Z\",\n        }\n\n    def _get_recognition_stats(self) -> dict:\n        \"\"\"Calculate recognition statistics\"\"\"\n        if not self._recognition_metrics:\n            return {\n                \"totalRecognitions\": 0,\n                \"averageConfidence\": 0.0,\n                \"averageProcessingTime\": 0.0,\n                \"recognitionRate\": 0.0,\n            }\n\n        total = len(self._recognition_metrics)\n        avg_confidence = mean(m.confidence for m in self._recognition_metrics)\n        avg_time = mean(m.processing_time for m in self._recognition_metrics)\n\n        # Recognition rate: percentage with confidence >= 0.85\n        successful = sum(1 for m in self._recognition_metrics if m.confidence >= 0.85)\n        rate = successful / total if total > 0 else 0.0\n\n        return {\n            \"totalRecognitions\": total,\n            \"averageConfidence\": round(avg_confidence, 2),\n            \"averageProcessingTime\": round(avg_time, 2),\n            \"recognitionRate\": round(rate, 2),\n        }\n\n    def _get_embedding_stats(self) -> dict:\n        \"\"\"Calculate embedding statistics\"\"\"\n        if not self._embedding_metrics:\n            return {\n                \"totalGenerated\": 0,\n                \"averageQuality\": 0.0,\n                \"averageProcessingTime\": 0.0,\n                \"successRate\": 0.0,\n            }\n\n        total = len(self._embedding_metrics)\n        successful = [m for m in self._embedding_metrics if m.success]\n\n        avg_quality = mean(m.quality for m in successful) if successful else 0.0\n        avg_time = mean(m.processing_time for m in self._embedding_metrics)\n        success_rate = len(successful) / total if total > 0 else 0.0\n\n        return {\n            \"totalGenerated\": len(successful),\n            \"averageQuality\": round(avg_quality, 2),\n            \"averageProcessingTime\": round(avg_time, 2),\n            \"successRate\": round(success_rate, 2),\n        }\n\n    def _get_camera_stats(self) -> dict:\n        \"\"\"Calculate camera statistics\"\"\"\n        if not self._camera_metrics:\n            return {\n                \"totalCameras\": 0,\n                \"activeCameras\": 0,\n                \"failedCameras\": 0,\n                \"averageFrameRate\": 0.0,\n            }\n\n        total = len(self._camera_metrics)\n        active = sum(1 for c in self._camera_metrics.values() if c.is_active)\n        failed = total - active\n\n        active_cameras = [c for c in self._camera_metrics.values() if c.is_active]\n        avg_fps = mean(c.frame_rate for c in active_cameras) if active_cameras else 0.0\n\n        return {\n            \"totalCameras\": total,\n            \"activeCameras\": active,\n            \"failedCameras\": failed,\n            \"averageFrameRate\": round(avg_fps, 1),\n        }\n\n\n# Global singleton instance\nmetrics_collector = MetricsCollector()\n"
    },
    {
      "path": "D:\\Work\\do_an_fall25\\fuacs\\recognition-service\\src\\recognition_service\\services\\model_loader.py",
      "relative_path": "src/recognition_service/services/model_loader.py",
      "filename": "model_loader.py",
      "size_bytes": 2953,
      "lines": 106,
      "last_modified": "2025-11-22T23:56:30.719967",
      "encoding": "utf-8",
      "language": "python",
      "content": "\"\"\"\nInsightFace Model Loader\n\nLoads and initializes InsightFace model on application startup.\nProvides global access to face analysis app.\n\nModel: buffalo_l\n- Detection: SCRFD (640x640)\n- Recognition: ArcFace ResNet100\n- Embedding dimension: 512\n\"\"\"\n\nimport insightface\nfrom insightface.app import FaceAnalysis\nimport logging\nfrom typing import Optional\nfrom recognition_service.core.config import get_settings\nfrom recognition_service.core.hardware import (\n    get_onnx_providers,\n    get_insightface_ctx_id,\n    get_device_info,\n)\n\nlogger = logging.getLogger(__name__)\n\n# Global face analysis app\nface_app: Optional[FaceAnalysis] = None\n\n\nasync def load_insightface_model() -> FaceAnalysis:\n    \"\"\"\n    Load InsightFace model asynchronously.\n\n    Downloads model on first run (~600MB) to:\n    - Default: ~/.insightface/models/buffalo_l/\n    - Custom: MODEL_PATH from settings\n\n    Returns:\n        FaceAnalysis: Loaded face analysis app\n\n    Raises:\n        RuntimeError: If model fails to load\n    \"\"\"\n    global face_app\n    settings = get_settings()\n\n    try:\n        # Initialize FaceAnalysis with auto-detected hardware config\n        providers = get_onnx_providers()\n        ctx_id = get_insightface_ctx_id()\n\n        face_app = FaceAnalysis(\n            name=settings.MODEL_NAME,\n            root=settings.MODEL_PATH,\n            providers=providers,\n        )\n\n        # Prepare model with auto-detected context\n        face_app.prepare(ctx_id=ctx_id, det_size=(640, 640))\n\n        logger.info(f\"InsightFace model loaded: {settings.MODEL_NAME}\")\n\n        return face_app\n\n    except Exception as e:\n        logger.error(f\"Failed to load InsightFace model: {e}\")\n        raise RuntimeError(f\"InsightFace model loading failed: {e}\")\n\n\ndef get_face_app() -> FaceAnalysis:\n    \"\"\"\n    Get loaded face analysis app with lazy loading.\n\n    Returns:\n        FaceAnalysis: Face analysis app\n\n    Raises:\n        RuntimeError: If model fails to load\n    \"\"\"\n    global face_app\n\n    if face_app is None:\n        logger.debug(\"Lazy loading InsightFace model...\")\n        settings = get_settings()\n\n        try:\n            # Initialize FaceAnalysis with auto-detected hardware config\n            providers = get_onnx_providers()\n            ctx_id = get_insightface_ctx_id()\n\n            face_app = FaceAnalysis(\n                name=settings.MODEL_NAME,\n                root=settings.MODEL_PATH,\n                providers=providers,\n            )\n\n            # Prepare model with auto-detected context\n            face_app.prepare(ctx_id=ctx_id, det_size=(640, 640))\n\n            logger.info(f\"InsightFace model loaded (lazy): {settings.MODEL_NAME}\")\n\n        except Exception as e:\n            logger.error(f\"Failed to load InsightFace model: {e}\")\n            raise RuntimeError(f\"InsightFace model loading failed: {e}\")\n\n    return face_app\n"
    },
    {
      "path": "D:\\Work\\do_an_fall25\\fuacs\\recognition-service\\src\\recognition_service\\services\\quality_analyzer.py",
      "relative_path": "src/recognition_service/services/quality_analyzer.py",
      "filename": "quality_analyzer.py",
      "size_bytes": 3368,
      "lines": 102,
      "last_modified": "2025-10-28T12:06:40.524255",
      "encoding": "utf-8",
      "language": "python",
      "content": "\"\"\"\nQuality Analyzer Service - Calculate face quality metrics\n\"\"\"\n\nimport cv2\nimport numpy as np\nimport logging\n\nlogger = logging.getLogger(__name__)\n\n\nclass QualityAnalyzer:\n    \"\"\"\n    Analyze face quality in frames.\n\n    Calculates 4 metrics:\n    1. Face Size - % of frame occupied by face\n    2. Clarity - Sharpness using Laplacian variance\n    3. Lighting - Brightness and contrast analysis\n    4. Face Angle - Frontal detection confidence\n    \"\"\"\n\n    def calculate_metrics(self, frame: np.ndarray, face) -> dict:\n        \"\"\"\n        Calculate quality metrics for a detected face.\n\n        Args:\n            frame: BGR image (numpy array)\n            face: Face object from InsightFace (has bbox, det_score, etc.)\n\n        Returns:\n            dict: {\n                'faceSize': 0.95,    # 0.0-1.0\n                'clarity': 0.88,     # 0.0-1.0\n                'lighting': 0.93,    # 0.0-1.0\n                'faceAngle': 0.92    # 0.0-1.0\n            }\n        \"\"\"\n        bbox = face.bbox.astype(int)\n        x1, y1, x2, y2 = bbox\n\n        # Ensure bbox is within frame bounds\n        frame_h, frame_w = frame.shape[:2]\n        x1, y1 = max(0, x1), max(0, y1)\n        x2, y2 = min(frame_w, x2), min(frame_h, y2)\n\n        # 1. Face Size Score\n        face_area = (x2 - x1) * (y2 - y1)\n        frame_area = frame_h * frame_w\n        face_size_ratio = face_area / frame_area\n        # Target: face should occupy >= 20% of frame\n        face_size_score = min(face_size_ratio / 0.20, 1.0)\n\n        # 2. Clarity Score (Laplacian variance for sharpness)\n        face_crop = frame[y1:y2, x1:x2]\n        gray = cv2.cvtColor(face_crop, cv2.COLOR_BGR2GRAY)\n        laplacian_var = cv2.Laplacian(gray, cv2.CV_64F).var()\n        # Threshold: 500 is good sharpness\n        clarity_score = min(laplacian_var / 500.0, 1.0)\n\n        # 3. Lighting Score (brightness + contrast)\n        brightness = np.mean(gray)\n        contrast = np.std(gray)\n        # Ideal brightness: 100-150 (out of 255)\n        brightness_score = 1.0 - abs(brightness - 125) / 125\n        # Ideal contrast: > 40\n        contrast_score = min(contrast / 50.0, 1.0)\n        lighting_score = (brightness_score + contrast_score) / 2\n\n        # 4. Face Angle Score (use detection confidence as proxy)\n        # Higher det_score = more frontal face\n        face_angle_score = min(face.det_score, 1.0)\n\n        metrics = {\n            \"faceSize\": round(float(face_size_score), 2),\n            \"clarity\": round(float(clarity_score), 2),\n            \"lighting\": round(float(lighting_score), 2),\n            \"faceAngle\": round(float(face_angle_score), 2),\n        }\n\n        logger.debug(f\"Quality metrics: {metrics}\")\n        return metrics\n\n    def calculate_overall_quality(self, metrics: dict) -> float:\n        \"\"\"\n        Calculate weighted overall quality score.\n\n        Weights:\n        - Face Size: 30%\n        - Clarity: 25%\n        - Lighting: 25%\n        - Face Angle: 20%\n\n        Args:\n            metrics: dict from calculate_metrics()\n\n        Returns:\n            float: Overall quality (0.0-1.0)\n        \"\"\"\n        weights = {\"faceSize\": 0.3, \"clarity\": 0.25, \"lighting\": 0.25, \"faceAngle\": 0.2}\n        quality = sum(metrics[k] * weights[k] for k in weights)\n        return round(quality, 2)\n"
    },
    {
      "path": "D:\\Work\\do_an_fall25\\fuacs\\recognition-service\\src\\recognition_service\\services\\recognition_service.py",
      "relative_path": "src/recognition_service/services/recognition_service.py",
      "filename": "recognition_service.py",
      "size_bytes": 13173,
      "lines": 311,
      "last_modified": "2025-11-23T00:23:02.961950",
      "encoding": "utf-8",
      "language": "python",
      "content": "\"\"\"\nRecognition Service\nCore business logic for face recognition sessions\n\"\"\"\n\nimport asyncio\nimport logging\nfrom datetime import datetime\nimport pytz\nfrom typing import Dict, List, Optional\nfrom recognition_service.models.recognition_requests import StartSessionRequest\nfrom recognition_service.models.recognition_responses import SessionDataDTO\nfrom recognition_service.services.session_manager import session_manager, SessionState\nfrom recognition_service.services.task_manager import task_manager\nfrom recognition_service.services.face_recognizer import face_recognizer\nfrom recognition_service.services.rtsp_handler import test_rtsp_connection\nfrom recognition_service.utils import get_utc_timestamp_for_java\nimport os\n\nlogger = logging.getLogger(__name__)\n\nclass RecognitionService:\n    \"\"\"\n    Manages face recognition sessions\n    \"\"\"\n\n    def __init__(self):\n        self.active_tasks: Dict[int, List[asyncio.Task]] = {}\n        # Track task IDs for each slot to properly stop them later\n        # Key: slot_id, Value: list of task_ids\n        self.task_ids: Dict[int, List[str]] = {}\n        # Track recognized students per session to prevent duplicate callbacks\n        # Key: slot_id, Value: set of student_user_ids\n        self.recognized_students: Dict[int, set] = {}\n\n    async def start_session(self, request: StartSessionRequest) -> SessionDataDTO:\n        \"\"\"\n        Start recognition session\n\n        Returns immediately after starting background tasks\n        \"\"\"\n        slot_id = request.slotId\n\n        # Check if session already exists (async for proper synchronization)\n        existing_session = await session_manager.get_session(slot_id)\n        if existing_session:\n            raise ValueError(f\"Session already exists for slot {slot_id}\")\n\n        # Test camera connections in parallel\n        camera_results = await self._test_cameras(request.cameras)\n\n        active_cameras = sum(1 for r in camera_results if r[\"connected\"])\n        failed_cameras = len(camera_results) - active_cameras\n\n        if active_cameras == 0:\n            raise RuntimeError(\"All cameras failed to connect\")\n\n        # Create evidence directory\n        evidence_dir = f\"./uploads/evidence/{slot_id}\"\n        os.makedirs(evidence_dir, exist_ok=True)\n\n        # Create session state\n        session_state = SessionState(\n            slot_id=slot_id,\n            room_id=request.roomId,\n            mode=request.mode.value,  # Store scan mode (INITIAL or RESCAN)\n            callback_type=request.callbackType,  # Store callback routing type\n            total_students=len(request.students),\n            total_cameras=len(request.cameras),\n            active_cameras=active_cameras,\n            failed_cameras=failed_cameras,\n            started_at=datetime.now(pytz.timezone('Asia/Ho_Chi_Minh')),\n            recognition_count=0\n        )\n\n        # Store session\n        await session_manager.add_session(slot_id, session_state)\n\n        # Start background tasks for each connected camera\n        tasks = []\n        task_ids = []\n        for i, camera in enumerate(request.cameras):\n            if camera_results[i][\"connected\"]:\n                task_id = f\"slot_{slot_id}_camera_{camera.id}\"\n                coro = self._process_camera(\n                    slot_id=slot_id,\n                    camera=camera,\n                    students=request.students,\n                    config=request.config\n                )\n                task = await task_manager.start_task(task_id, coro)\n                tasks.append(task)\n                task_ids.append(task_id)  # Store task ID for cleanup\n\n        self.active_tasks[slot_id] = tasks\n        self.task_ids[slot_id] = task_ids  # Store task IDs for cleanup\n\n        logger.info(f\"SESSION_START | slot={slot_id} room={request.roomId} cameras={len(tasks)} students={len(request.students)} mode={request.mode.value}\")\n\n        # Return session info\n        return SessionDataDTO(\n            slotId=slot_id,\n            roomId=request.roomId,\n            mode=request.mode.value,  # Include scan mode in response\n            totalStudents=len(request.students),\n            totalCameras=len(request.cameras),\n            activeCameras=active_cameras,\n            failedCameras=failed_cameras,\n            sessionStartedAt=get_utc_timestamp_for_java()\n        )\n\n    async def stop_session(self, slot_id: int) -> Optional[SessionDataDTO]:\n        \"\"\"\n        Stop recognition session and return statistics\n        \"\"\"\n        # Get session (async for proper synchronization)\n        session_state = await session_manager.get_session(slot_id)\n        if not session_state:\n            return None\n\n        # Stop all tasks\n        if slot_id in self.active_tasks:\n            for task in self.active_tasks[slot_id]:\n                if not task.done():\n                    task.cancel()\n\n            # Wait for cancellation\n            await asyncio.gather(*self.active_tasks[slot_id], return_exceptions=True)\n            del self.active_tasks[slot_id]\n\n        # Stop task manager tasks using the stored task IDs\n        if slot_id in self.task_ids:\n            for task_id in self.task_ids[slot_id]:\n                await task_manager.stop_task(task_id)\n            del self.task_ids[slot_id]\n\n        # Calculate duration\n        duration = int((datetime.now(pytz.timezone('Asia/Ho_Chi_Minh')) - session_state.started_at).total_seconds())\n\n        # Extract recognized student IDs BEFORE cleanup (for RESCAN mode Case 1)\n        recognized_student_ids = None\n        if slot_id in self.recognized_students:\n            recognized_student_ids = list(self.recognized_students[slot_id])\n\n        # Remove from session manager\n        await session_manager.remove_session(slot_id)\n\n        # Clean up recognized students tracking\n        if slot_id in self.recognized_students:\n            del self.recognized_students[slot_id]\n\n        logger.info(f\"SESSION_STOP | slot={slot_id} duration={duration}s recognitions={session_state.recognition_count}\")\n\n        return SessionDataDTO(\n            slotId=slot_id,\n            roomId=session_state.room_id,\n            mode=session_state.mode,  # Include scan mode in response\n            totalStudents=session_state.total_students,\n            totalCameras=session_state.total_cameras,\n            activeCameras=session_state.active_cameras,\n            failedCameras=session_state.failed_cameras,\n            sessionStoppedAt=get_utc_timestamp_for_java(),\n            sessionDuration=duration,\n            totalRecognitions=session_state.recognition_count,\n            recognizedStudentIds=recognized_student_ids  # Include for RESCAN mode Case 1\n        )\n\n    async def _test_cameras(self, cameras) -> List[Dict]:\n        \"\"\"Test all cameras in parallel\"\"\"\n        tasks = []\n        for camera in cameras:\n            tasks.append(test_rtsp_connection(camera.rtspUrl))\n\n        results = await asyncio.gather(*tasks, return_exceptions=True)\n\n        camera_results = []\n        for i, result in enumerate(results):\n            if isinstance(result, Exception):\n                camera_results.append({\"connected\": False, \"error\": str(result)})\n                logger.warning(f\"CAMERA_FAIL | camera={cameras[i].id} name={cameras[i].name} error={result}\")\n            else:\n                camera_results.append(result)\n                if result[\"connected\"]:\n                    logger.debug(f\"Camera {cameras[i].name} connected\")\n\n        return camera_results\n\n    async def _process_camera(self, slot_id: int, camera, students, config):\n        \"\"\"\n        Background task to process camera stream\n\n        Runs continuously until cancelled\n        \"\"\"\n        from recognition_service.services.callback_service import callback_service\n        import cv2\n        import numpy as np\n\n        logger.info(f\"CAMERA_START | slot={slot_id} camera={camera.id}\")\n\n        try:\n            cap = cv2.VideoCapture(camera.rtspUrl)\n            cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # Minimize buffer to get latest frame\n            if not cap.isOpened():\n                logger.error(f\"Failed to open camera {camera.id}\")\n                return\n\n            scan_interval = config.scanInterval\n            last_scan = datetime.now(pytz.timezone('Asia/Ho_Chi_Minh'))\n            frame_count = 0\n            processed_count = 0\n\n            logger.info(f\"CAMERA_LOOP | slot={slot_id} camera={camera.id} interval={scan_interval}s\")\n\n            while True:\n                # Check if cancelled\n                if asyncio.current_task().cancelled():\n                    break\n\n                # Flush old frames from buffer to get latest frame\n                # This reduces lag and ensures we capture the most recent moment\n                for _ in range(3):\n                    cap.grab()\n\n                # Read frame (now getting the latest one)\n                ret, frame = cap.read()\n                frame_count += 1\n\n                if not ret or frame is None:\n                    logger.warning(f\"Failed to read frame from camera {camera.id} (frame #{frame_count})\")\n                    await asyncio.sleep(1)\n                    continue\n\n                # Check scan interval\n                now = datetime.now(pytz.timezone('Asia/Ho_Chi_Minh'))\n                if (now - last_scan).total_seconds() < scan_interval:\n                    await asyncio.sleep(0.1)\n                    continue\n\n                last_scan = now\n                processed_count += 1\n\n                # Log every 10th scan to reduce noise\n                if processed_count % 10 == 1:\n                    logger.debug(f\"SCAN | slot={slot_id} camera={camera.id} scan_no={processed_count}\")\n\n                # Initialize tracking set for this slot if not exists\n                if slot_id not in self.recognized_students:\n                    self.recognized_students[slot_id] = set()\n\n                # Get scan mode and callback type from session (needed for both processing and callbacks)\n                session = await session_manager.get_session(slot_id)\n                scan_mode = session.mode if session else \"INITIAL\"\n                callback_type = session.callback_type if session else \"REGULAR\"\n\n                # Process frame for face recognition (with deduplication at face_recognizer level)\n                recognitions = await face_recognizer.process_frame(\n                    frame=frame,\n                    students=students,\n                    similarity_threshold=config.similarityThreshold,\n                    slot_id=slot_id,\n                    camera_id=camera.id,\n                    recognized_students=self.recognized_students[slot_id],\n                    callback_type=callback_type  # Pass callback type to determine evidence routing\n                )\n\n                # Send callbacks for recognized faces (already deduplicated by face_recognizer)\n                if recognitions:\n\n                    for recognition in recognitions:\n                        student_id = recognition['studentUserId']\n\n                        # Add to tracking set\n                        self.recognized_students[slot_id].add(student_id)\n\n                        # Send callback with scan mode and callback type\n                        await callback_service.send_recognition(\n                            callback_url=config.callbackUrl,\n                            slot_id=slot_id,\n                            recognition=recognition,\n                            mode=scan_mode,  # Include scan mode in callback\n                            callback_type=callback_type  # Include callback routing type\n                        )\n\n                        # Update counter\n                        await session_manager.increment_recognition_count(slot_id)\n\n                        # Log with detailed context for easy filtering\n                        logger.info(\n                            f\"RECOGNITION | slot={slot_id} camera={camera.id} \"\n                            f\"student={recognition.get('rollNumber', student_id)} \"\n                            f\"confidence={recognition.get('confidence', 0):.2f} \"\n                            f\"mode={scan_mode}\"\n                        )\n\n                    logger.info(f\"BATCH_SENT | slot={slot_id} camera={camera.id} count={len(recognitions)}\")\n\n                # Small delay to prevent CPU overload\n                await asyncio.sleep(0.1)\n\n        except asyncio.CancelledError:\n            logger.info(f\"CAMERA_CANCEL | slot={slot_id} camera={camera.id}\")\n            raise\n        except Exception as e:\n            logger.error(f\"CAMERA_ERROR | slot={slot_id} camera={camera.id} error={str(e)}\")\n        finally:\n            if 'cap' in locals():\n                cap.release()\n            logger.info(f\"CAMERA_STOP | slot={slot_id} camera={camera.id}\")\n\n# Singleton instance\nrecognition_service = RecognitionService()\n"
    },
    {
      "path": "D:\\Work\\do_an_fall25\\fuacs\\recognition-service\\src\\recognition_service\\services\\rtsp_handler.py",
      "relative_path": "src/recognition_service/services/rtsp_handler.py",
      "filename": "rtsp_handler.py",
      "size_bytes": 5179,
      "lines": 163,
      "last_modified": "2025-11-22T23:49:21.829248",
      "encoding": "utf-8",
      "language": "python",
      "content": "\"\"\"\nRTSP Stream Handler\n\nHandles RTSP camera connections using OpenCV.\nProvides frame capture, FPS calculation, and connection testing.\n\"\"\"\n\nimport os\nimport logging\nimport time\nfrom typing import Optional, Tuple, List\nimport numpy as np\n\n# Force TCP transport for RTSP streams for better reliability\nos.environ[\"OPENCV_FFMPEG_CAPTURE_OPTIONS\"] = \"rtsp_transport;tcp\"\n\nimport cv2\n\nlogger = logging.getLogger(__name__)\n\n\nclass RTSPConnectionError(Exception):\n    \"\"\"Raised when RTSP connection fails\"\"\"\n\n    pass\n\n\nclass RTSPHandler:\n    \"\"\"\n    RTSP stream connection handler using OpenCV.\n\n    Usage:\n        with RTSPHandler(\"rtsp://...\") as handler:\n            frames, fps = handler.capture_frames(10)\n    \"\"\"\n\n    def __init__(self, rtsp_url: str, timeout: int = 5):\n        self.rtsp_url = rtsp_url\n        self.timeout = timeout\n        self.cap: Optional[cv2.VideoCapture] = None\n        self._is_connected = False\n\n    def connect(self) -> bool:\n        \"\"\"Establish RTSP connection.\"\"\"\n        logger.debug(f\"Connecting to RTSP: {self.rtsp_url}\")\n\n        try:\n            self.cap = cv2.VideoCapture(self.rtsp_url, cv2.CAP_FFMPEG)\n            self.cap.set(cv2.CAP_PROP_OPEN_TIMEOUT_MSEC, self.timeout * 1000)\n            self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)  # Minimize buffer to get latest frame\n\n            # Verify by reading first frame\n            ret, frame = self.cap.read()\n            if not ret or frame is None:\n                raise RTSPConnectionError(\"Failed to read frame\")\n\n            self._is_connected = True\n            logger.debug(f\"RTSP connected successfully\")\n            return True\n\n        except Exception as e:\n            if self.cap:\n                self.cap.release()\n            logger.error(f\"RTSP connection failed: {str(e)}\")\n            raise RTSPConnectionError(str(e))\n\n    def capture_frames(self, count: int = 10) -> Tuple[List[np.ndarray], float]:\n        \"\"\"Capture multiple frames and calculate FPS.\"\"\"\n        if not self._is_connected:\n            raise RTSPConnectionError(\"Not connected\")\n\n        frames = []\n        start_time = time.time()\n\n        logger.debug(f\"Capturing {count} frames for FPS calculation\")\n        for i in range(count):\n            ret, frame = self.cap.read()\n            if ret and frame is not None:\n                frames.append(frame)\n\n        elapsed = time.time() - start_time\n        fps = len(frames) / elapsed if elapsed > 0 else 0.0\n\n        logger.debug(f\"Captured {len(frames)} frames in {elapsed:.2f}s, FPS: {fps:.1f}\")\n        return frames, fps\n\n    def capture_single_frame(self) -> np.ndarray:\n        \"\"\"Capture one frame.\"\"\"\n        if not self._is_connected:\n            raise RTSPConnectionError(\"Not connected\")\n\n        ret, frame = self.cap.read()\n        if not ret or frame is None:\n            raise RTSPConnectionError(\"Failed to capture frame\")\n\n        return frame\n\n    def get_resolution(self) -> Tuple[int, int]:\n        \"\"\"Get video resolution (width, height).\"\"\"\n        if not self._is_connected:\n            raise RTSPConnectionError(\"Not connected\")\n\n        width = int(self.cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n        height = int(self.cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n        return width, height\n\n    def calculate_latency(self) -> int:\n        \"\"\"Calculate approximate latency in milliseconds.\"\"\"\n        start = time.time()\n        try:\n            self.capture_single_frame()\n            latency = int((time.time() - start) * 1000)\n            logger.debug(f\"Measured latency: {latency}ms\")\n            return latency\n        except Exception as e:\n            logger.warning(f\"Failed to measure latency: {e}\")\n            return 0\n\n    def release(self):\n        \"\"\"Release resources.\"\"\"\n        if self.cap:\n            self.cap.release()\n            self._is_connected = False\n            logger.debug(\"RTSP connection released\")\n\n    def __enter__(self):\n        self.connect()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.release()\n\n\nasync def test_rtsp_connection(rtsp_url: str, timeout: int = 5) -> dict:\n    \"\"\"\n    Test RTSP connection and return result dictionary\n    \n    Args:\n        rtsp_url: RTSP stream URL\n        timeout: Connection timeout in seconds\n        \n    Returns:\n        Dict with connection status and details\n    \"\"\"\n    try:\n        with RTSPHandler(rtsp_url, timeout) as handler:\n            width, height = handler.get_resolution()\n            frames, fps = handler.capture_frames(5)\n            latency = handler.calculate_latency()\n            \n            return {\n                \"connected\": True,\n                \"frameRate\": fps,\n                \"resolution\": {\"width\": width, \"height\": height},\n                \"latency\": latency,\n                \"stability\": \"stable\" if fps > 10 else \"unstable\"\n            }\n    except Exception as e:\n        logger.error(f\"RTSP connection test failed for {rtsp_url}: {e}\")\n        return {\n            \"connected\": False,\n            \"error\": str(e)\n        }"
    },
    {
      "path": "D:\\Work\\do_an_fall25\\fuacs\\recognition-service\\src\\recognition_service\\services\\session_manager.py",
      "relative_path": "src/recognition_service/services/session_manager.py",
      "filename": "session_manager.py",
      "size_bytes": 3775,
      "lines": 127,
      "last_modified": "2025-11-04T12:12:18.854217",
      "encoding": "utf-8",
      "language": "python",
      "content": "\"\"\"\nSession Manager\n\nManages active face recognition sessions in-memory.\nProvides thread-safe operations for adding, removing, and querying sessions.\n\nNote: Sessions are stored in RAM and will be lost on service restart.\nFor production, consider persisting to Redis or database.\n\"\"\"\n\nfrom dataclasses import dataclass, field\nfrom datetime import datetime\nfrom typing import Dict, Optional, List\nimport asyncio\n\n\n@dataclass\nclass SessionState:\n    \"\"\"\n    Represents an active face recognition session.\n\n    Attributes:\n        slot_id: Unique slot identifier\n        room_id: Room where session is running\n        mode: Scan mode (INITIAL or RESCAN)\n        callback_type: Callback routing type (\"REGULAR\" or \"EXAM\")\n        total_students: Number of students enrolled\n        total_cameras: Total cameras configured\n        active_cameras: Cameras successfully connected\n        failed_cameras: Cameras that failed to connect\n        started_at: Session start timestamp\n        recognition_count: Number of recognitions performed\n    \"\"\"\n\n    slot_id: int\n    room_id: int\n    mode: str = \"INITIAL\"\n    callback_type: str = \"REGULAR\"  # Routes callback to correct backend table\n    total_students: int = 0\n    total_cameras: int = 0\n    active_cameras: int = 0\n    failed_cameras: int = 0\n    started_at: datetime = field(default_factory=datetime.now)\n    recognition_count: int = 0\n\n\nclass SessionManager:\n    \"\"\"\n    Manages face recognition sessions.\n\n    Thread-safe singleton for tracking active sessions across the application.\n    \"\"\"\n\n    def __init__(self):\n        self._sessions: Dict[int, SessionState] = {}\n        self._lock = asyncio.Lock()\n\n    async def add_session(self, slot_id: int, state: SessionState) -> None:\n        \"\"\"\n        Add a new session.\n\n        Args:\n            slot_id: Slot ID\n            state: Session state\n\n        Raises:\n            ValueError: If session already exists\n        \"\"\"\n        async with self._lock:\n            if slot_id in self._sessions:\n                raise ValueError(f\"Session already exists for slot {slot_id}\")\n            self._sessions[slot_id] = state\n\n    async def remove_session(self, slot_id: int) -> Optional[SessionState]:\n        \"\"\"\n        Remove and return session.\n\n        Args:\n            slot_id: Slot ID\n\n        Returns:\n            SessionState if found, None otherwise\n        \"\"\"\n        async with self._lock:\n            return self._sessions.pop(slot_id, None)\n\n    async def get_session(self, slot_id: int) -> Optional[SessionState]:\n        \"\"\"\n        Get session by slot ID (async for consistency with add/remove).\n\n        Args:\n            slot_id: Slot ID\n\n        Returns:\n            SessionState if found, None otherwise\n        \"\"\"\n        async with self._lock:\n            return self._sessions.get(slot_id)\n\n    async def increment_recognition_count(self, slot_id: int) -> None:\n        \"\"\"\n        Increment recognition counter for session.\n\n        Args:\n            slot_id: Slot ID\n        \"\"\"\n        async with self._lock:\n            if slot_id in self._sessions:\n                self._sessions[slot_id].recognition_count += 1\n\n    @property\n    def active_sessions_count(self) -> int:\n        \"\"\"Get number of active sessions\"\"\"\n        return len(self._sessions)\n\n    @property\n    def active_sessions(self) -> Dict[int, SessionState]:\n        \"\"\"Get copy of all active sessions\"\"\"\n        return self._sessions.copy()\n\n    def get_all_sessions(self) -> List[SessionState]:\n        \"\"\"Get list of all session states\"\"\"\n        return list(self._sessions.values())\n\n\n# Global singleton instance\nsession_manager = SessionManager()\n"
    },
    {
      "path": "D:\\Work\\do_an_fall25\\fuacs\\recognition-service\\src\\recognition_service\\services\\task_manager.py",
      "relative_path": "src/recognition_service/services/task_manager.py",
      "filename": "task_manager.py",
      "size_bytes": 2827,
      "lines": 108,
      "last_modified": "2025-11-22T23:48:56.109302",
      "encoding": "utf-8",
      "language": "python",
      "content": "\"\"\"\nTask Manager\nManages background tasks for camera processing\n\"\"\"\n\nimport asyncio\nimport logging\nfrom typing import Dict, Optional\n\nlogger = logging.getLogger(__name__)\n\nclass TaskManager:\n    \"\"\"\n    Manages async background tasks\n    \"\"\"\n\n    def __init__(self):\n        self._tasks: Dict[str, asyncio.Task] = {}\n        self._lock = asyncio.Lock()\n\n    async def start_task(self, task_id: str, coro) -> asyncio.Task:\n        \"\"\"\n        Start a new background task\n\n        Args:\n            task_id: Unique task identifier\n            coro: Coroutine to run\n\n        Returns:\n            The created task\n\n        Raises:\n            ValueError: If task already exists\n        \"\"\"\n        async with self._lock:\n            if task_id in self._tasks:\n                raise ValueError(f\"Task {task_id} already exists\")\n\n            task = asyncio.create_task(coro)\n            self._tasks[task_id] = task\n\n            logger.debug(f\"Task started: {task_id}\")\n            return task\n\n    async def stop_task(self, task_id: str) -> bool:\n        \"\"\"\n        Stop a background task\n\n        Args:\n            task_id: Task to stop\n\n        Returns:\n            True if task was stopped, False if not found\n        \"\"\"\n        async with self._lock:\n            if task_id not in self._tasks:\n                return False\n\n            task = self._tasks[task_id]\n\n            if not task.done():\n                task.cancel()\n                try:\n                    await task\n                except asyncio.CancelledError:\n                    pass\n\n            del self._tasks[task_id]\n            logger.debug(f\"Task stopped: {task_id}\")\n            return True\n\n    async def stop_all_tasks(self) -> int:\n        \"\"\"\n        Stop all running tasks\n\n        Returns:\n            Number of tasks stopped\n        \"\"\"\n        async with self._lock:\n            count = 0\n\n            for task_id, task in list(self._tasks.items()):\n                if not task.done():\n                    task.cancel()\n                    count += 1\n\n            # Wait for all cancellations\n            if self._tasks:\n                await asyncio.gather(\n                    *self._tasks.values(),\n                    return_exceptions=True\n                )\n\n            self._tasks.clear()\n            logger.info(f\"Stopped all tasks ({count} tasks)\")\n            return count\n\n    def get_task(self, task_id: str) -> Optional[asyncio.Task]:\n        \"\"\"Get task by ID\"\"\"\n        return self._tasks.get(task_id)\n\n    @property\n    def active_tasks_count(self) -> int:\n        \"\"\"Get number of active tasks\"\"\"\n        return len([t for t in self._tasks.values() if not t.done()])\n\n# Singleton instance\ntask_manager = TaskManager()\n"
    },
    {
      "path": "D:\\Work\\do_an_fall25\\fuacs\\recognition-service\\src\\recognition_service\\services\\video_processor.py",
      "relative_path": "src/recognition_service/services/video_processor.py",
      "filename": "video_processor.py",
      "size_bytes": 5072,
      "lines": 153,
      "last_modified": "2025-10-28T12:06:40.778123",
      "encoding": "utf-8",
      "language": "python",
      "content": "\"\"\"\nVideo Processor Service - Video quality analysis\n\"\"\"\n\nimport cv2\nimport numpy as np\nimport logging\nfrom recognition_service.services.face_encoder import FaceEncoder\nfrom recognition_service.services.quality_analyzer import QualityAnalyzer\nfrom recognition_service.core.config import get_settings\n\nlogger = logging.getLogger(__name__)\n\n\nclass VideoProcessor:\n    \"\"\"\n    Process videos for quality analysis.\n\n    Used by /embeddings/validate endpoint.\n    \"\"\"\n\n    def __init__(self):\n        self.face_encoder = FaceEncoder()\n        self.quality_analyzer = QualityAnalyzer()\n        self.settings = get_settings()\n\n    async def analyze_quality(self, video_path: str) -> dict:\n        \"\"\"\n        Analyze video quality for face detection.\n\n        Process:\n        1. Extract frames (every 1 second)\n        2. Detect faces in each frame\n        3. Calculate quality metrics per frame\n        4. Average metrics across all frames\n        5. Generate user-friendly feedback\n\n        Args:\n            video_path: Path to video file\n\n        Returns:\n            dict: {\n                'quality': 0.92,\n                'faceDetected': True,\n                'isAcceptable': True,  # True if >= 0.70\n                'metrics': {faceSize, clarity, lighting, faceAngle},\n                'feedback': [\"Face is clearly visible\", ...]\n            }\n        \"\"\"\n        logger.info(f\"Analyzing video quality: {video_path}\")\n\n        cap = cv2.VideoCapture(video_path)\n        if not cap.isOpened():\n            raise ValueError(f\"Cannot open video: {video_path}\")\n\n        fps = cap.get(cv2.CAP_PROP_FPS)\n        interval = self.settings.VALIDATION_SAMPLE_INTERVAL\n        frame_interval = int(fps * interval)  # Sample every 1 second\n\n        frame_count = 0\n        metrics_list = []\n\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n\n            # Sample at interval\n            if frame_count % frame_interval == 0:\n                # Detect faces\n                faces = self.face_encoder.detect_faces(frame)\n\n                if len(faces) > 0:\n                    face = faces[0]  # Use first face\n                    metrics = self.quality_analyzer.calculate_metrics(frame, face)\n                    metrics_list.append(metrics)\n\n            frame_count += 1\n\n        cap.release()\n\n        # No face detected\n        if len(metrics_list) == 0:\n            logger.warning(\"No face detected in any frame\")\n            return {\n                \"quality\": 0.0,\n                \"faceDetected\": False,\n                \"isAcceptable\": False,\n                \"metrics\": {\"faceSize\": 0.0, \"clarity\": 0.0, \"lighting\": 0.0, \"faceAngle\": 0.0},\n                \"feedback\": [\"No face detected in video\"],\n            }\n\n        # Average metrics\n        avg_metrics = self._average_metrics(metrics_list)\n\n        # Overall quality\n        quality = self.quality_analyzer.calculate_overall_quality(avg_metrics)\n\n        # Generate feedback\n        feedback = self._generate_feedback(avg_metrics)\n\n        logger.info(\n            f\"Video analysis complete: quality={quality}, frames_analyzed={len(metrics_list)}\"\n        )\n\n        return {\n            \"quality\": quality,\n            \"faceDetected\": True,\n            \"isAcceptable\": quality >= 0.70,\n            \"metrics\": avg_metrics,\n            \"feedback\": feedback,\n        }\n\n    def _average_metrics(self, metrics_list: list) -> dict:\n        \"\"\"Average metrics across all frames\"\"\"\n        keys = [\"faceSize\", \"clarity\", \"lighting\", \"faceAngle\"]\n        avg = {}\n        for key in keys:\n            values = [m[key] for m in metrics_list]\n            avg[key] = round(float(np.mean(values)), 2)\n        return avg\n\n    def _generate_feedback(self, metrics: dict) -> list[str]:\n        \"\"\"Generate actionable feedback messages\"\"\"\n        feedback = []\n\n        # Face Size feedback\n        if metrics[\"faceSize\"] >= 0.8:\n            feedback.append(\"Face is clearly visible\")\n        elif metrics[\"faceSize\"] >= 0.5:\n            feedback.append(\"Face size is acceptable\")\n        else:\n            feedback.append(\"Face is too small. Move closer to camera\")\n\n        # Clarity feedback\n        if metrics[\"clarity\"] >= 0.8:\n            feedback.append(\"Video is clear and sharp\")\n        elif metrics[\"clarity\"] < 0.6:\n            feedback.append(\"Video is slightly blurry. Hold camera steady\")\n\n        # Lighting feedback\n        if metrics[\"lighting\"] >= 0.8:\n            feedback.append(\"Good lighting conditions\")\n        elif metrics[\"lighting\"] < 0.6:\n            feedback.append(\"Lighting is too dark. Record in brighter environment\")\n\n        # Face Angle feedback\n        if metrics[\"faceAngle\"] >= 0.8:\n            feedback.append(\"Face is properly centered\")\n        elif metrics[\"faceAngle\"] < 0.6:\n            feedback.append(\"Face angle is not frontal. Look directly at camera\")\n\n        return feedback\n"
    }
  ]
}